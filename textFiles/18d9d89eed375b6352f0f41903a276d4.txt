













Covariance - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Covariance";
		var wgTitle = "Covariance";
		var wgAction = "view";
		var wgArticleId = "157059";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 280459622;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Covariance

From Wikipedia, the free encyclopedia

Jump to: navigation, search 
This article is about the measure of correlation between random variables.  For other uses, see Covariance (disambiguation).
In probability theory and statistics, covariance is a measure of how much two variables change together (variance is a special case of the covariance when the two variables are identical).
If two variables tend to vary together (that is, when one of them is above its expected value, then the other variable tends to be above its expected value too), then the covariance between the two variables will be positive. On the other hand, if one of them tends to be above its expected value when the other variable is below its expected value, then the covariance between the two variables will be negative.




Contents


1 Definition
2 Properties

2.1 Incremental computation
2.2 Relationship to inner products


3 Covariance matrix, operator, bilinear form, and function
4 Comments
5 See also
6 External links





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Definition
The covariance between two real-valued random variables X and Y, with expected values  and  is defined as



where E is the expected value operator. This can also be written:





Random variables whose covariance is zero are called uncorrelated.
If X and Y are independent, then their covariance is zero. This follows because under independence,



Recalling the final form of the covariance derivation given above, and substituting, we get



The converse, however, is generally not true: Some pairs of random variables have covariance zero although they are not independent. Under some additional assumptions, covariance zero sometimes does entail independence, as for example in the case of multivariate normal distributions.
The units of measurement of the covariance Cov(X, Y) are those of X times those of Y. By contrast, correlation, which depends on the covariance, is a dimensionless measure of linear dependence.

[edit] Properties
If X, Y, W, and V are real-valued random variables and a, b, c, d are constant ("constant" in this context means non-random), then the following facts are a consequence of the definition of covariance:








For sequences X1, ..., Xn and Y1, ..., Ym of random variables, we have



For a sequence X1, ..., Xn of random variables, and constants a1, ..., an, we have




[edit] Incremental computation
Covariance can be computed efficiently from incrementally available values using a generalization of the computational formula for the variance:




[edit] Relationship to inner products
Many of the properties of covariance can be extracted elegantly by observing that it satisfies similar properties to those of an inner product:

(1) bilinear: for constants a and b and random variables X, Y, and U, Cov(aX + bY, U) = a Cov(X, U) + b Cov(Y, U)
(2) symmetric: Cov(X, Y) = Cov(Y, X)
(3) positive semi-definite: Var(X) = Cov(X, X) ≥ 0, and Cov(X, X) = 0 implies that X is a constant random variable (K).

It can be shown that the covariance is an inner product over some subspace of the vector space of random variables with finite second moment.

[edit] Covariance matrix, operator, bilinear form, and function
For column-vector valued random variables X and Y with respective expected values μ and ν, and respective scalar components m and n, the covariance is defined to be the m×n matrix called the covariance matrix:



For vector-valued random variables, Cov(X, Y) and Cov(Y, X) are each other's transposes.
More generally, for a probability measure P on a Hilbert space H with inner product , the covariance of P is the bilinear form Cov: H × H → H given by



for all x and y in H. The covariance operator C is then defined by



(from the Riesz representation theorem, such operator exists if Cov is bounded). Since Cov is symmetric in its arguments, the covariance operator is self-adjoint (the infinite-dimensional analogy of the transposition symmetry in the finite-dimensional case). When P is a centred Gaussian measure, C is also a nuclear operator. In particular, it is a compact operator of trace class, that is, it has finite trace.
Even more generally, for a probability measure P on a Banach space B, the covariance of P is the bilinear form on the algebraic dual , defined by



where  is now the value of the linear functional x on the element z.
Quite similarly, the covariance function of a function-valued random element (in special cases called random process or random field) z is



where z(x) is now the value of the function z at the point x, i.e., the value of the linear functional  evaluated at z.

[edit] Comments
The covariance is sometimes called a measure of "linear dependence" between the two random variables. That does not mean the same thing as in the context of linear algebra (see linear dependence). When the covariance is normalized, one obtains the correlation matrix. From it, one can obtain the Pearson coefficient, which gives us the goodness of the fit for the best possible linear function describing the relation between the variables. In this sense covariance is a linear gauge of dependence.

[edit] See also




Look up covariance in Wiktionary, the free dictionary.


Covariance function
Covariance matrix
Autocovariance
Analysis of covariance
Sample mean and sample covariance


[edit] External links

MathWorld page on calculating the sample covariance








v • d • e

Statistics





Design of experiments

Population • Sampling • Stratified sampling • Replication • Blocking






Sample size estimation

Null hypothesis • Alternative hypothesis • Type I and Type II errors • Statistical power • Effect size • Standard error






Descriptive statistics





Continuous data






Location


Mean (Arithmetic, Geometric, Harmonic) • Median • Mode







Dispersion


Range • Standard deviation • Coefficient of variation • Percentile







Moments


Variance • Semivariance • Skewness • Kurtosis










Categorical data


Frequency • Contingency table









Inferential statistics

Bayesian inference • Frequentist inference • Hypothesis testing • Significance • P-value • Interval estimation • Confidence interval • Meta-analysis






General estimation

Bayesian estimator • Maximum likelihood • Method of moments • Minimum distance • Maximum spacing






Specific tests

Z-test(normal) • Student's t-test • Chi-square test • F-test • Sensitivity and specificity






Survival analysis

Survival function • Kaplan-Meier • Logrank test • Failure rate • Proportional hazards models






Correlation

Pearson product-moment correlation coefficient • Rank correlation (Spearman's rho, Kendall's tau) • Confounding variable






Linear models

General linear model • Generalized linear model • Analysis of variance • Analysis of covariance






Regression analysis

Linear regression • Nonlinear regression • Nonparametric regression • Semiparametric regression • Logistic regression






Statistical graphics

Bar chart • Biplot • Box plot • Control chart • Forest plot • Histogram • Q-Q plot • Run chart • Scatter plot • Stemplot






History

History of statistics • Founders of statistics • Timeline of probability and statistics






Publications

Journals in statistics • Important publications






Category • Portal • Topic outline • List of topics








Retrieved from "http://en.wikipedia.org/wiki/Covariance"
Categories: Covariance and correlationHidden categories: Statistics articles with navigational template 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


العربية
Česky
Dansk
Deutsch
Español
Esperanto
Euskara
Français
한국어
Italiano
עברית
Magyar
Nederlands
日本語
‪Norsk (bokmål)‬
Polski
Português
Русский
Basa Sunda
Suomi
Svenska
Tiếng Việt
Türkçe
Українська
中文









 This page was last modified on 29 March 2009, at 18:00.
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
