













Chebyshev's inequality - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Chebyshev\'s_inequality";
		var wgTitle = "Chebyshev\'s inequality";
		var wgAction = "view";
		var wgArticleId = "156533";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 278812182;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Chebyshev's inequality

From Wikipedia, the free encyclopedia

Jump to: navigation, search 
For the similarly named inequality involving series, see Chebyshev's sum inequality.
In probability theory, Chebyshev's inequality (also known as Tchebysheff's inequality, Chebyshev's theorem, or the Bienaymé-Chebyshev inequality) states that in any data sample or probability distribution, nearly all the values are close to the mean value, and provides a quantitative description of "nearly all" and "close to".
More precisely, no more than 1/k2 of the values are more than k standard deviations away from the mean.
Chebyshev's inequality is named for the Russian mathematician Pafnuty Chebyshev, although it was first formulated by his friend and colleague Irénée-Jules Bienaymé.[1]




Contents


1 General statement

1.1 Measure-theoretic statement
1.2 Probabilistic statement
1.3 Example


2 Variant: One-sided Chebyshev inequality
3 Proof (of the two-sided Chebyshev's inequality)

3.1 Measure-theoretic proof
3.2 Probabilistic proof


4 See also
5 References
6 Further reading





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] General statement
The inequality can be stated quite generally using measure theory; the statement in the language of probability theory then follows as a particular case, for a space of measure 1.

[edit] Measure-theoretic statement
Let (X,Σ,μ) be a measure space, and let f be an extended real-valued measurable function defined on X. Then for any real number t > 0,



More generally, if g is a nonnegative extended real-valued measurable function, nondecreasing on the range of f, then



The previous statement then follows by defining g(t) as



and taking |f| instead of f.

[edit] Probabilistic statement
Let X be a random variable with expected value μ and finite variance σ2. Then for any real number k > 0,



Only the cases k > 1 provide useful information. This can be equivalently stated as



As an example, using k = √2 shows that at least half of the values lie in the interval (μ − √2 σ, μ + √2 σ).
Typically, the theorem will provide rather loose bounds. However, the bounds provided by Chebyshev's inequality cannot, in general (remaining sound for variables of arbitrary distribution), be improved upon. For example, for any k > 1, the following example (where σ = 1/k) meets the bounds exactly.



For this distribution,



Equality holds exactly for any distribution that is a linear transformation of this one. Inequality holds for any distribution that is not a linear transformation of this one.
The theorem can be useful despite loose bounds because it applies to random variables of any distribution, and because these bounds can be calculated knowing no more about the distribution than the mean and variance.
Chebyshev's inequality is used for proving the weak law of large numbers.

[edit] Example
For illustration, assume we have a large body of text, for example articles from a publication. Assume we know that the articles are on average 1000 characters long with a standard deviation of 200 characters. From Chebyshev's inequality we can then infer that the chance that a given article is between 600 and 1400 characters would be at least 75% (k = 2).
The inequality is coarse: a more accurate guess would be possible if the distribution of the length of the articles is known. For example, a normal distribution would yield a 75% chance of an article being between 770 and 1230 characters long.

[edit] Variant: One-sided Chebyshev inequality





It has been suggested that An inequality on location and scale parameters be merged into this article or section. (Discuss)


A one-tailed variant with k > 0, is[2]



The one-sided version of the Chebyshev inequality is called Cantelli's inequality, and is due to Francesco Paolo Cantelli.

[edit] Proof (of the two-sided Chebyshev's inequality)

[edit] Measure-theoretic proof
Let  be defined as , and let  be the indicator function of the set . Then, it is easy to check that



and therefore,



The desired inequality follows from dividing the above inequality by g(t).

[edit] Probabilistic proof
Markov's inequality states that for any real-valued random variable Y and any positive number a, we have Pr(|Y| > a) ≤ E(|Y|)/a. One way to prove Chebyshev's inequality is to apply Markov's inequality to the random variable Y = (X − μ)2 with a = (σk)2.
It can also be proved directly. For any event A, let IA be the indicator random variable of A, i.e. IA equals 1 if A occurs and 0 otherwise. Then










The direct proof shows why the bounds are quite loose in typical cases: the number 1 to the left of "≥" is replaced by [(X − μ)/(kσ)]2 to the right of "≥" whenever the latter exceeds 1. In some cases it exceeds 1 by a very wide margin.

[edit] See also

Markov's inequality
A stronger result applicable to unimodal probability distributions is the Vysochanskiï-Petunin inequality.
Proof of the weak law of large numbers where Chebyshev's inequality is used.
Table of mathematical symbols
Multidimensional Chebyshev's inequality


[edit] References


^ Donald Knuth, "The Art of Computer Programming", 3rd edition, volume 1, 1997, p.98
^ Grimmett and Stirzaker, problem 7.11.9. Several proofs of this result can be found here.



[edit] Further reading

A. Papoulis (1991), Probability, Random Variables, and Stochastic Processes, 3rd ed. McGraw-Hill. ISBN 0-07-100870-5. pp. 113-114.
G. Grimmett and D. Stirzaker (2001), Probability and Random Processes, 3rd ed. Oxford. ISBN 0 19 857222 0. Section 7.3.




Retrieved from "http://en.wikipedia.org/wiki/Chebyshev%27s_inequality"
Categories: Probabilistic inequalities | Probability theory | Articles containing proofsHidden categories: Articles to be merged since June 2008 | All articles to be merged 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Česky
Deutsch
Español
Français
한국어
Italiano
עברית
日本語
Македонски
‪Norsk (bokmål)‬
Polski
Português
Русский
Suomi
ไทย
اردو
中文









 This page was last modified on 21 March 2009, at 21:27 (UTC).
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
