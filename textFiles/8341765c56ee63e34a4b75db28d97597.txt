













Mahalanobis distance - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Mahalanobis_distance";
		var wgTitle = "Mahalanobis distance";
		var wgAction = "view";
		var wgArticleId = "799760";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 268658146;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Mahalanobis distance

From Wikipedia, the free encyclopedia

Jump to: navigation, search 
In statistics, Mahalanobis distance is a distance measure introduced by P. C. Mahalanobis in 1936.[1] It is based on correlations between variables by which different patterns can be identified and analyzed. It is a useful way of determining similarity of an unknown sample set to a known one. It differs from Euclidean distance in that it takes into account the correlations of the data set and is scale-invariant, i.e. not dependent on the scale of measurements.




Contents


1 Definition
2 Intuitive explanation
3 Relationship to leverage
4 Applications
5 References





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Definition
Formally, the Mahalanobis distance from a group of values with mean  and covariance matrix S for a multivariate vector  is defined as:

[2]

Mahalanobis distance (or "generalized squared interpoint distance" for its squared value[3]) can also be defined as dissimilarity measure between two random vectors  and  of the same distribution with the covariance matrix S :



If the covariance matrix is the identity matrix, the Mahalanobis distance reduces to the Euclidean distance. If the covariance matrix is diagonal, then the resulting distance measure is called the normalized Euclidean distance:



where σi is the standard deviation of the xi over the sample set.

[edit] Intuitive explanation
Consider the problem of estimating the probability that a test point in N-dimensional Euclidean space belongs to a set, where we are given sample points that definitely belong to that set. Our first step would be to find the average or center of mass of the sample points. Intuitively, the closer the point in question is to this center of mass, the more likely it is to belong to the set.
However, we also need to know how large the set is. The simplistic approach is to estimate the standard deviation of the distances of the sample points from the center of mass. If the distance between the test point and the center of mass is less than one standard deviation, then we conclude that it is highly probable that the test point belongs to the set. The further away it is, the more likely that the test point should not be classified as belonging to the set.
This intuitive approach can be made quantitative by defining the normalized distance between the test point and the set to be . By plugging this into the normal distribution we get the probability of the test point belonging to the set.
The drawback of the above approach was that we assumed that the sample points are distributed about the center of mass in a spherical manner. Were the distribution to be decidedly non-spherical, for instance ellipsoidal, then we would expect the probability of the test point belonging to the set to depend not only on the distance from the center of mass, but also on the direction. In those directions where the ellipsoid has a short axis the test point must be closer, while in those where the axis is long the test point can be further away from the center.
Putting this on a mathematical basis, the ellipsoid that best represents the set's probability distribution can be estimated by building the covariance matrix of the samples. The Mahalanobis distance is simply the distance of the test point from the center of mass divided by the width of the ellipsoid in the direction of the test point.
This last property, of minimizing the distance between a test point and the mean, is common to all Bregman divergences, of which the Mahalanobis distance is an example.

[edit] Relationship to leverage
Mahalanobis distance is closely related to the leverage statistic, h, but has a different scale:[4]

Mahalanobis distance = (N − 1)(h − 1/N).


[edit] Applications
Mahalanobis' discovery was prompted by the problem of identifying the similarities of skulls based on measurements in 1927.[5]
Mahalanobis distance is widely used in cluster analysis and other classification techniques. It is closely related to Hotelling's T-square distribution used for multivariate statistical testing and Fisher's Linear Discriminant Analysis that is used for supervised classification.[6]
In order to use the Mahalanobis distance to classify a test point as belonging to one of N classes, one first estimates the covariance matrix of each class, usually based on samples known to belong to each class. Then, given a test sample, one computes the Mahalanobis distance to each class, and classifies the test point as belonging to that class for which the Mahalanobis distance is minimal. Using the probabilistic interpretation given above, this is equivalent to selecting the class with the maximum likelihood.[citation needed]
Also, Mahalanobis distance and leverage are often used to detect outliers, especially in the development of linear regression models. A point that has a greater Mahalanobis distance from the rest of the sample population of points is said to have higher leverage since it has a greater influence on the slope or coefficients of the regression equation. Mahalanobis distance is also used to determine multivariate outliers. Regression techniques can be used to determine if a specific case within a sample population is an outlier via the combination of two or more variable scores. A case need not be a univariate outlier on a variable to be a multivariate outlier. The significance of a Mahalanobis distance when detecting multivariate outliers is evaluated as a Chi Square with k degrees of freedom.[citation needed]

[edit] References


^ Mahalanobis, P C (1936). "On the generalised distance in statistics". Proceedings of the National Institute of Sciences of India 2 (1): 49–55. http://ir.isical.ac.in/dspace/handle/1/1268. Retrieved on 2008-11-05. 
^ De Maesschalck, R.; D. Jouan-Rimbaud, D.L. Massart (2000) The Mahalanobis distance. Chemometrics and Intelligent Laboratory Systems 50:1–18
^ Gnanadesikan, R., and J.R. Kettenring (1972). Robust estimates, residuals, and outlier detection with multiresponse data. Biometrics 28:81-124.
^ Schinka, J. A., Velicer, W. F., & Weiner, I. B. (2003). Research methods in psychology. Wiley.
^ Mahalanobis, P. C. (1927). Analysis of race mixture in Bengal. J. Proc. Asiatic Soc. of Bengal. 23:301-333.
^ McLachlan, Geoffry J (1992) Discriminant Analysis and Statistical Pattern Recognition. Wiley Interscience. ISBN 0471691151 p. 12





Retrieved from "http://en.wikipedia.org/wiki/Mahalanobis_distance"
Categories: Statistical distance measures | Statistical terminology | Multivariate statisticsHidden categories: All articles with unsourced statements | Articles with unsourced statements since December 2008 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Català
Deutsch
Español
Français
日本語
Nederlands
Polski
Português
Svenska
中文









 This page was last modified on 5 February 2009, at 09:21.
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
