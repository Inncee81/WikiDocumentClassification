













Poisson distribution - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Poisson_distribution";
		var wgTitle = "Poisson distribution";
		var wgAction = "view";
		var wgArticleId = "41561";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 281344616;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Poisson distribution

From Wikipedia, the free encyclopedia

Jump to: navigation, search 

Poisson

Probability mass function

The horizontal axis is the index k. The function is only defined at integer values of k (empty lozenges). The connecting lines are only guides for the eye.


Cumulative distribution function

The horizontal axis is the index k. The CDF is discontinuous at the integers of k and flat everywhere else because a variable that is Poisson distributed only takes on integer values.


Parameters



Support



Probability mass function (pmf)



Cumulative distribution function (cdf)
 for  or 
(where  is the Incomplete gamma function and  is the floor function)



Mean



Median



Mode
 and λ − 1 if λ is an integer


Variance



Skewness



Excess kurtosis



Entropy

(for large λ) 
                    



Moment-generating function (mgf)



Characteristic function



In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a number of events occurring in a fixed period of time if these events occur with a known average rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.
The distribution was discovered by Siméon-Denis Poisson (1781–1840) and published, together with his probability theory, in 1838 in his work Recherches sur la probabilité des jugements en matières criminelles et matière civile ("Research on the Probability of Judgments in Criminal and Civil Matters"). The work focused on certain random variables N that count, among other things, the number of discrete occurrences (sometimes called "arrivals") that take place during a time-interval of given length. If the expected number of occurrences in this interval is λ, then the probability that there are exactly k occurrences (k being a non-negative integer, k = 0, 1, 2, ...) is equal to



where

e is the base of the natural logarithm (e = 2.71828...)
k is the number of occurrences of an event - the probability of which is given by the function
k! is the factorial of k
λ is a positive real number, equal to the expected number of occurrences that occur during the given interval. For instance, if the events occur on average 4 times per minute, and you are interested in the number of events occurring in a 10 minute interval, you would use as your model a Poisson distribution with λ = 10×4 = 40.

As a function of k, this is the probability mass function. The Poisson distribution can be derived as a limiting case of the binomial distribution.
The Poisson distribution can be applied to systems with a large number of possible events, each of which is rare. A classic example is the nuclear decay of atoms.
The Poisson distribution is sometimes called a Poissonian, analogous to the term Gaussian for a Gauss or normal distribution.




Contents


1 Poisson noise and characterizing small occurrences
2 Related distributions
3 Occurrence
4 How does this distribution arise? — The law of rare events
5 Properties

5.1 Generating Poisson-distributed random variables


6 Parameter estimation

6.1 Maximum likelihood
6.2 Bayesian inference


7 The "law of small numbers"
8 See also
9 Online visualization tools
10 Notes
11 References
12 External links





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Poisson noise and characterizing small occurrences
The parameter λ is not only the mean number of occurrences , but also its variance  (see Table). Thus, the number of observed occurrences fluctuates about its mean λ with a standard deviation . These fluctuations are denoted as Poisson noise or (particularly in electronics) as shot noise.
The correlation of the mean and standard deviation in counting independent, discrete occurrences is useful scientifically. By monitoring how the fluctuations vary with the mean signal, one can estimate the contribution of a single occurrence, even if that contribution is too small to be detected directly. For example, the charge e on an electron can be estimated by correlating the magnitude of an electric current with its shot noise. If N electrons pass a point in a given time t on the average, the mean current is I = eN / t; since the current fluctuations should be of the order  (i.e. the standard deviation of the Poisson process), the charge e can be estimated from the ratio . An everyday example is the graininess that appears as photographs are enlarged; the graininess is due to Poisson fluctuations in the number of reduced silver grains, not to the individual grains themselves. By correlating the graininess with the degree of enlargement, one can estimate the contribution of an individual grain (which is otherwise too small to be seen unaided). Many other molecular applications of Poisson noise have been developed, e.g., estimating the number density of receptor molecules in a cell membrane.




[edit] Related distributions

If  and  then the difference Y = X1 − X2 follows a Skellam distribution.
If  and  are independent, and Y = X1 + X2, then the distribution of X1 conditional on Y = y is a binomial. Specifically, . More generally, if X1, X2,..., Xn are independent Poisson random variables with parameters λ1, λ2,..., λn then









The Poisson distribution can be derived as a limiting case to the binomial distribution as the number of trials goes to infinity and the expected number of successes remains fixed. Therefore it can be used as an approximation of the binomial distribution if n is sufficiently large and p is sufficiently small. There is a rule of thumb stating that the Poisson distribution is a good approximation of the binomial distribution if n is at least 20 and p is smaller than or equal to 0.05. According to this rule the approximation is excellent if n ≥ 100 and np ≤ 10.[1]
For sufficiently large values of λ, (say λ>1000), the normal distribution with mean λ, and variance λ, is an excellent approximation to the Poisson distribution. If λ is greater than about 10, then the normal distribution is a good approximation if an appropriate continuity correction is performed, i.e., P(X ≤ x), where (lower-case) x is a non-negative integer, is replaced by P(X ≤ x + 0.5).









Variance stabilizing transformation: When a variable is Poisson distributed, its square root is approximately normally distributed with expected value of about  and variance of about 1/4.[2] Under this transformation, the convergence to normality is far faster than the untransformed variable. Other, slightly more complicated, variance stabilizing transformations are available,[3] one of which is Anscombe transform. See Data transformation (statistics) for more general uses of transformations.
If the number of arrivals in a given time interval [0,t] follows the Poisson distribution, with mean = λt, then the lengths of the inter-arrival times follow the Exponential distribution, with mean 1 / λ.


[edit] Occurrence
The Poisson distribution arises in connection with Poisson processes. It applies to various phenomena of discrete nature (that is, those that may happen 0, 1, 2, 3, ... times during a given period of time or in a given area) whenever the probability of the phenomenon happening is constant in time or space. Examples of events that may be modelled as a Poisson distribution include:

The number of soldiers killed by horse-kicks each year in each corps in the Prussian cavalry. This example was made famous by a book of Ladislaus Josephovich Bortkiewicz (1868–1931).
The number of phone calls at a call centre per minute.
Under an assumption of homogeneity, the number of times a web server is accessed per minute.
The number of mutations in a given stretch of DNA after a certain amount of radiation.


[edit] How does this distribution arise? — The law of rare events
In several of the above examples—for example, the number of mutations in a given sequence of DNA—the events being counted are actually the outcomes of discrete trials, and would more precisely be modelled using the binomial distribution. However, the binomial distribution with parameters n and λ/n, i.e., the probability distribution of the number of successes in n trials, with probability λ/n of success on each trial, approaches the Poisson distribution with expected value λ as n approaches infinity. This provides a means by which to approximate random variables using the Poisson distribution rather than the more-cumbersome binomial distribution.
This limit is sometimes known as the law of rare events, since each of the individual Bernoulli events rarely triggers. The name may be misleading because the total count of success events in a Poisson process need not be rare if the parameter λ is not small. For example, the number of telephone calls to a busy switchboard in one hour follows a Poisson distribution with the events appearing frequent to the operator, but they are rare from the point of the average member of the population who is very unlikely to make a call to that switchboard in that hour.
The proof may proceed as follows. First, recall from calculus



and the definition of the Binomial distribution



If the binomial probability can be defined such that p = λ / n, we can evaluate the limit of P as n goes large:



The F term can be written as



and then note that, since k is fixed, this is a rational function of n with limit 1.
Consequently, the limit of the distribution becomes



which now assumes the Poisson distribution.
More generally, whenever a sequence of independent binomial random variables with parameters n and pn is such that



the sequence converges in distribution to a Poisson random variable with mean λ (see, e.g. law of rare events).

[edit] Properties

The expected value of a Poisson-distributed random variable is equal to λ and so is its variance. The higher moments of the Poisson distribution are Touchard polynomials in λ, whose coefficients have a combinatorial meaning. In fact, when the expected value of the Poisson distribution is 1, then Dobinski's formula says that the nth moment equals the number of partitions of a set of size n.


The mode of a Poisson-distributed random variable with non-integer λ is equal to , which is the largest integer less than or equal to λ. This is also written as floor(λ). When λ is a positive integer, the modes are λ and λ − 1.


Sums of Poisson-distributed random variables:


If  follow a Poisson distribution with parameter  and Xi are independent, then




also follows a Poisson distribution whose parameter is the sum of the component parameters.


The moment-generating function of the Poisson distribution with expected value λ is









All of the cumulants of the Poisson distribution are equal to the expected value λ. The nth factorial moment of the Poisson distribution is λn.


The Poisson distributions are infinitely divisible probability distributions.


The directed Kullback-Leibler divergence between Pois(λ) and Pois(λ0) is given by









[edit] Generating Poisson-distributed random variables
A simple way to generate random Poisson-distributed numbers is given by Knuth, see References below.

algorithm poisson random number (Knuth):
    init:
         Let L ← e−λ, k ← 0 and p ← 1.
    do:
         k ← k + 1.
         Generate uniform random number u in [0,1] and let p ← p × u.
    while p ≥ L.
    return k − 1. 

While simple, the complexity is linear in λ. There are many other algorithms to overcome this. Some are given in Ahrens & Dieter, see References below.

[edit] Parameter estimation

[edit] Maximum likelihood
Given a sample of n measured values ki we wish to estimate the value of the parameter λ of the Poisson population from which the sample was drawn. To calculate the maximum likelihood value, we form the log-likelihood function



Take the derivative of L with respect to λ and equate it to zero:



Solving for λ yields the maximum-likelihood estimate of λ:



Since each observation has expectation λ so does this sample mean. Therefore it is an unbiased estimator of λ. It is also an efficient estimator, i.e. its estimation variance achieves the Cramér-Rao lower bound (CRLB).

[edit] Bayesian inference
In Bayesian inference, the conjugate prior for the rate parameter λ of the Poisson distribution is the Gamma distribution. Let



denote that λ is distributed according to the Gamma density g parameterized in terms of a shape parameter α and an inverse scale parameter β:



Then, given the same sample of n measured values ki as before, and a prior of Gamma(α, β), the posterior distribution is



The posterior mean E[λ] approaches the maximum likelihood estimate  in the limit as .
The posterior predictive distribution of additional data is a Gamma-Poisson (i.e. negative binomial) distribution.

[edit] The "law of small numbers"
The word law is sometimes used as a synonym of probability distribution, and convergence in law means convergence in distribution. Accordingly, the Poisson distribution is sometimes called the law of small numbers because it is the probability distribution of the number of occurrences of an event that happens rarely but has very many opportunities to happen. The Law of Small Numbers is a book by Ladislaus Bortkiewicz about the Poisson distribution, published in 1898. Some historians of mathematics have argued that the Poisson distribution should have been called the Bortkiewicz distribution.[4]

[edit] See also

Compound Poisson distribution
Tweedie distributions
Poisson process
Poisson regression
Poisson sampling
Queueing theory
Erlang distribution which describes the waiting time until n events have occurred. For temporally distributed events, the Poisson distribution is the probability distribution of the number of events that would occur within a preset time, the Erlang distribution is the probability distribution of the amount of time until the nth event.
Skellam distribution, the distribution of the difference of two Poisson variates, not necessarily from the same parent distribution.
Incomplete gamma function used to calculate the CDF.
Dobinski's formula (on combinatorial interpretation of the moments of the Poisson distribution)
Schwarz formula
Robbins lemma, a lemma relevant to empirical Bayes methods relying on the Poisson distribution
Coefficient of dispersion, a simple measure to assess whether observed events are close to Poisson


[edit] Online visualization tools

SOCR distribution GUI
TAMU's Interactive Poisson Distribution


[edit] Notes

^ NIST/SEMATECH, '6.3.3.1. Counts Control Charts', e-Handbook of Statistical Methods, accessed 25 October 2006
^ McCullagh, Peter; Nelder, John (1989). Generalized Linear Models. London: Chapman and Hall. ISBN 0-412-31760-5.  page 196 gives the approximation and the subsequent terms.
^ Johnson, N.L., Kotz, S., Kemp, A.W. (1993) Univariate Discrete distributions (2nd edition). Wiley. ISBN 0-471-54897-9, p163
^ p.e. I J Good, Some statistical applications of Poisson's work, Statist. Sci. 1 (2) (1986), 157-180. JSTOR link


[edit] References

Donald E. Knuth (1969). Seminumerical Algorithms. The Art of Computer Programming, Volume 2. Addison Wesley. 


Joachim H. Ahrens, Ulrich Dieter (1974). "Computer Methods for Sampling from Gamma, Beta, Poisson and Binomial Distributions". Computing 12 (3): 223--246. doi:10.1007/BF02293108. 


Joachim H. Ahrens, Ulrich Dieter (1982). "Computer Generation of Poisson Deviates". ACM Transactions on Mathematical Software 8 (2): 163--179. doi:10.1145/355993.355997. 


Ronald J. Evans, J. Boersma, N. M. Blachman, A. A. Jagers (1988). "The Entropy of a Poisson Distribution: Problem 87-6". SIAM Review 30 (2): 314--317. doi:10.1137/1030059. 


[edit] External links

Poisson Distribution at QWiki








v • d • e

Probability distributions










 
Discrete univariate with finite support






Benford · Bernoulli · binomial · categorical · hypergeometric · Rademacher · discrete uniform · Zipf · Zipf-Mandelbrot














 
Discrete univariate with infinite support






Boltzmann · Conway-Maxwell-Poisson · compound Poisson · discrete phase-type · extended negative binomial · Gauss-Kuzmin · geometric · logarithmic · negative binomial · parabolic fractal · Poisson · Skellam · Yule-Simon · zeta














 
Continuous univariate supported on a bounded interval, e.g. [0,1]






Beta · Irwin-Hall · Kumaraswamy · raised cosine · triangular · U-quadratic · uniform · Wigner semicircle














 
Continuous univariate supported on a semi-infinite interval, usually [0,∞)






Beta prime · Bose–Einstein · Burr · chi-square · chi · Coxian · Erlang · exponential · F · Fermi-Dirac · folded normal · Fréchet · Gamma · generalized extreme value · generalized inverse Gaussian · half-logistic · half-normal · Hotelling's T-square · hyper-exponential · hypoexponential · inverse chi-square (scaled inverse chi-square) · inverse Gaussian · inverse gamma · Lévy · log-normal · log-logistic · Maxwell-Boltzmann · Maxwell speed · Nakagami · noncentral chi-square · Pareto · phase-type · Rayleigh · relativistic Breit–Wigner · Rice · Rosin–Rammler · shifted Gompertz · truncated normal · type-2 Gumbel · Weibull · Wilks' lambda














 
Continuous univariate supported on the whole real line (-∞,∞)






Cauchy · extreme value · exponential power · Fisher's z  · generalized normal  · generalized hyperbolic  · Gumbel · hyperbolic secant · Landau · Laplace · logistic · normal (Gaussian) · normal inverse Gaussian · skew normal · stable · Student's t · type-1 Gumbel · Variance-Gamma · Voigt














 
Multivariate (joint)






Discrete: Ewens · Beta-binomial · multinomial · multivariate Polya
Continuous: Dirichlet · Generalized Dirichlet · multivariate normal · multivariate Student  · normal-scaled inverse gamma  · normal-gamma
Matrix-valued: inverse-Wishart · matrix normal · Wishart














 
Directional, degenerate, and singular






Directional: Kent  · von Mises · von Mises–Fisher
Degenerate: discrete degenerate · Dirac delta function
Singular: Cantor














 
Families






exponential · natural exponential · location-scale · maximum entropy · Pearson · Tweedie
















v • d • e

Statistics





Design of experiments

Population • Sampling • Stratified sampling • Replication • Blocking






Sample size estimation

Null hypothesis • Alternative hypothesis • Type I and Type II errors • Statistical power • Effect size • Standard error






Descriptive statistics





Continuous data






Location


Mean (Arithmetic, Geometric, Harmonic) • Median • Mode







Dispersion


Range • Standard deviation • Coefficient of variation • Percentile







Moments


Variance • Semivariance • Skewness • Kurtosis










Categorical data


Frequency • Contingency table









Inferential statistics

Bayesian inference • Frequentist inference • Hypothesis testing • Significance • P-value • Interval estimation • Confidence interval • Meta-analysis






General estimation

Bayesian estimator • Maximum likelihood • Method of moments • Minimum distance • Maximum spacing






Specific tests

Z-test (normal) • Student's t-test • Chi-square test • F-test • Sensitivity and specificity






Survival analysis

Survival function • Kaplan-Meier • Logrank test • Failure rate • Proportional hazards models






Correlation

Pearson product-moment correlation coefficient • Rank correlation (Spearman's rho, Kendall's tau) • Confounding variable






Linear models

General linear model • Generalized linear model • Analysis of variance • Analysis of covariance






Regression analysis

Linear regression • Nonlinear regression • Nonparametric regression • Semiparametric regression • Logistic regression






Statistical graphics

Bar chart • Biplot • Box plot • Control chart • Forest plot • Histogram • Q-Q plot • Run chart • Scatter plot • Stemplot






History

History of statistics • Founders of statistics • Timeline of probability and statistics






Publications

Journals in statistics • Important publications






Category • Portal • Topic outline • List of topics








Retrieved from "http://en.wikipedia.org/wiki/Poisson_distribution"
Categories: Discrete distributions | Factorial and binomial topics | Articles with example pseudocodeHidden categories: Statistics articles with navigational template 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


العربية
Български
Česky
Deutsch
Español
فارسی
Français
한국어
Italiano
עברית
Lietuvių
Magyar
Nederlands
日本語
‪Norsk (bokmål)‬
Novial
Polski
Português
Русский
Simple English
Basa Sunda
Suomi
Svenska
Tiếng Việt
Türkçe
Українська
中文









 This page was last modified on 2 April 2009, at 19:10.
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
