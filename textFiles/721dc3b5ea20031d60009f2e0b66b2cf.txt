













Benchmark (computing) - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Benchmark_(computing)";
		var wgTitle = "Benchmark (computing)";
		var wgAction = "view";
		var wgArticleId = "1980870";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 281082872;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Benchmark (computing)

From Wikipedia, the free encyclopedia

Jump to: navigation, search 

This article is about the use of benchmarks in computing, for other uses see benchmark.

In computing, a benchmark is the act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it. The term 'benchmark' is also mostly utilized for the purposes of elaborately-designed benchmarking programs themselves. Benchmarking is usually associated with assessing performance characteristics of computer hardware, for example, the floating point operation performance of a CPU, but there are circumstances when the technique is also applicable to software. Software benchmarks are, for example, run against compilers or database management systems. Another type of test program, namely test suites or validation suites, are intended to assess the correctness of software.
Benchmarks provide a method of comparing the performance of various subsystems across different chip/system architectures.




Software Testing portal







Contents


1 Purpose
2 Challenges
3 Types of benchmarks
4 Common benchmarks

4.1 Industry standard (audited and verifiable)
4.2 Open source benchmarks
4.3 Microsoft Windows benchmarks
4.4 Others


5 See also
6 References
7 Further reading
8 External links





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Purpose
As computer architecture advanced, it became more difficult to compare the performance of various computer systems simply by looking at their specifications. Therefore, tests were developed that allowed comparison of different architectures. For example, Pentium 4 processors generally operate at a higher clock frequency than Athlon XP processors, which does not necessarily translate to more computational power. A slower processor, with regard to clock frequency, can perform as well as a processor operating at a higher frequency. See BogoMips and the megahertz myth.
Benchmarks are designed to mimic a particular type of workload on a component or system. Synthetic benchmarks do this by specially created programs that impose the workload on the component. Application benchmarks run real-world programs on the system. Whilst application benchmarks usually give a much better measure of real-world performance on a given system, synthetic benchmarks are useful for testing individual components, like a hard disk or networking device.
Benchmarks are particularly important in CPU design, giving processor architects the ability to measure and make tradeoffs in microarchitectural decisions. For example, if a benchmark extracts the key algorithms of an application, it will contain the performance-sensitive aspects of that application. Running this much smaller snippet on a cycle-accurate simulator, can give clues on how to improve performance.
Prior to 2000, computer and microprocessor architects used SPEC to do this, although SPEC's Unix-based benchmarks were quite lengthy and thus unwieldy to use intact.
Computer manufacturers are known to configure their systems to give unrealistically high performance on benchmark tests that are not replicated in real usage. For instance, during the 1980s some compilers could detect a specific mathematical operation used in a well-known floating-point benchmark and replace the operation with a faster mathematically-equivalent operation. However, such a transformation was rarely useful outside the benchmark until the mid-1990s, when RISC and VLIW architectures emphasized the importance of compiler technology as it related to performance. Benchmarks are now regularly used by compiler companies to improve not only their own benchmark scores, but real application performance.
CPUs that have many execution units — such as a superscalar CPU, a VLIW CPU, or a reconfigurable computing CPU — typically have slower clock rates than a sequential CPU with one or two execution units, when built from transistors that are just as fast. Nevertheless, CPUs with many execution units often complete real-world and benchmark tasks in less time than the supposedly faster high-clock-rate CPU.
Given the large number of benchmarks available, a manufacturer can usually find at least one benchmark that shows its system will outperform another system; the other systems can be shown to excel with a different benchmark.
Manufacturers commonly report only those benchmarks (or aspects of benchmarks) that show their products in the best light. They also have been known to mis-represent the significance of benchmarks, again to show their products in the best possible light. Taken together, these practices are called bench-marketing.
Ideally benchmarks should only substitute for real applications if the application is unavailable, or too difficult or costly to port to a specific processor or computer system. If performance is critical, the only benchmark that matters is the intended workload.

[edit] Challenges
Benchmarking is not easy and often involves several iterative rounds in order to arrive at predictable, useful conclusions. Interpretation of benchmarking data is also extraordinarily difficult. Here is a partial list of common challenges:

Vendors tend to tune their products specifically for industry-standard benchmarks. Norton SysInfo (SI) is particularly easy to tune for, since it mainly biased toward the speed of multiple operations. Use extreme caution in interpreting such results.
Many benchmarks focus entirely on the speed of computational performance, neglecting other important features of a computer system, such as:

Qualities of service, aside from raw performance. Examples of unmeasured qualities of service include security, availability, reliability, execution integrity, serviceability, scalability (especially the ability to quickly and nondisruptively add or reallocate capacity), etc. There are often real trade-offs between and among these qualities of service, and all are important in business computing. Transaction Processing Performance Council Benchmark specifications partially address these concerns by specifying ACID property tests, database scalability rules, and service level requirements.
In general, benchmarks do not measure Total cost of ownership. Transaction Processing Performance Council Benchmark specifications partially address this concern by specifying that a price/performance metric must be reported in addition to a raw performance metric, using a simplified TCO formula.
Electrical power. When more power is used, a portable system will have a shorter battery life and require recharging more often. This is often the antithesis of performance as most semiconductors require more power to switch faster. See also performance per watt.
In some embedded systems, where memory is a significant cost, better code density can significantly reduce costs.


Benchmarks seldom measure real world performance of mixed workloads — running multiple applications concurrently in a full, multi-department or multi-application business context. For example, IBM's mainframe servers (System z9) excel at mixed workload, but industry-standard benchmarks don't tend to measure the strong I/O and large and fast memory design such servers require. (Most other server architectures dictate fixed-function (single-purpose) deployments, e.g. "database servers" and "Web application servers" and "file servers," and measure only that. The better question is, "What more computing infrastructure would I need to fully support all this extra workload?")
Vendor benchmarks tend to ignore requirements for development, test, and disaster recovery computing capacity. Vendors only like to report what might be narrowly required for production capacity in order to make their initial acquisition price seem as low as possible.
Benchmarks are having trouble adapting to widely distributed servers, particularly those with extra sensitivity to network topologies. The emergence of grid computing, in particular, complicates benchmarking since some workloads are "grid friendly", while others are not.
Users can have very different perceptions of performance than benchmarks may suggest. In particular, users appreciate predictability — servers that always meet or exceed service level agreements. Benchmarks tend to emphasize mean scores (IT perspective) rather than low standard deviations (user perspective).
Many server architectures degrade dramatically at high (near 100%) levels of usage — "fall off a cliff" — and benchmarks should (but often do not) take that factor into account. Vendors, in particular, tend to publish server benchmarks at continuous at about 80% usage — an unrealistic situation — and do not document what happens to the overall system when demand spikes beyond that level.
Benchmarking institutions often disregard or do not follow basic scientific method. This includes, but is not limited to: small sample size, lack of variable control, and the limited repeatability of results.[1]


[edit] Types of benchmarks

Real program

word processing software
tool software of CDA
user's application software (MIS)


Kernel

contains key codes
normally abstracted from actual program
popular kernel: Livermore loop
linpack benchmark (contains basic linear algebra subroutine written in FORTRAN language)
results are represented in MFLOPS


Component Benchmark/ micro-benchmark

programs designed to measure performance of a computer's basic components [2]
automatic detection of computer's hardware parameters like number of registers, cache size, memory latency


Synthetic Benchmark

Procedure for programming synthetic Bench mark

take statistics of all type of operations from plenty of application programs
get proportion of each operation
write a program based on the proportion above


Types of Synthetic Benchmark are:

Whetstone
Dhrystone


These were the first general purpose industry standard computer benchmarks. They do not necessarily obtain high scores on modern pipelined computers.


I/O benchmarks
Parallel benchmarks:- used on machines with multiple processors or systems consisting of multiple machines.


[edit] Common benchmarks

[edit] Industry standard (audited and verifiable)

Business Applications Performance Corporation (BAPCo)
Embedded Microprocessor Benchmark Consortium (EEMBC)
Standard Performance Evaluation Corporation (SPEC)
Transaction Processing Performance Council (TPC)


[edit] Open source benchmarks

DEISA Benchmark Suite: scientific HPC applications benchmark
Dhrystone: integer arithmetic performance
Fhourstones: an integer benchmark
HINT: It ranks a computer system as a whole.
Iometer: I/O subsystem measurement and characterization tool for single and clustered systems.
Linpack / LAPACK
Livermore loops
NAS parallel benchmarks
PAL: a benchmark for realtime physics engines
Phoronix Test Suite: open-source benchmarking suite for Linux, OpenSolaris, and FreeBSD
POV-Ray: 3D render
TPoX: An XML transaction processing benchmark for XML databases
Ubench: A simple cpu and memory benchmark for various flavors of Unix (including Linux).
Whetstone: floating-point arithmetic performance
LMBench: Suite of simple, portable benchmarks, useful for comparing performance of different UNIX systems[3]


[edit] Microsoft Windows benchmarks

BAPCo: MobileMark, SYSmark, WebMark
Futuremark:3DMark, PCMark
Whetstone
PiFast
SuperPrime
Super PI
WinSAT, exclusively for Windows Vista, providing an index for consumers to rate their systems easily


[edit] Others

BRL-CAD
Khornerstone
iCOMP, the Intel comparative microprocessor performance, published by Intel
Performance Rating, modelling scheme used by AMD and Cyrix to reflect the relative performance usually compared to competing products
VMmark: a VMware-only[4] server virtualization benchmark suite.


[edit] See also

Benchmarking (business perspective)
Test suite a collection of test cases intended to show that a software program has some specified set of behaviors
Figure of merit


[edit] References

^ "Hardware Testing and Benchmarking Methodology". 2006. http://donutey.com/hardwaretesting.php. Retrieved on 2008-02-24. 
^ Micro-benchmark
^ "LMbench - Tools for Performance Analysis". 2009. http://www.bitmover.com/lmbench/. Retrieved on 2009-02-12. 
^ http://www.vmware.com.mx/pdf/VMmark_Rules_1.1.1_20080611-1.pdf


[edit] Further reading

Jim Gray (Editor), The Benchmark Handbook for Database and Transaction Systems (2nd Edition), Morgan Kaufmann, 1993, ISBN 1-55860-292-5
Bert Scalzo, Kevin Kline, Claudia Fernandez, Donald K. Burleson, Mike Ault (2007), Database Benchmarking Practical Methods for Oracle & SQL Server. ISBN 0-9776715-3-4


[edit] External links

benchmark newsgroup
benchmark webpage
CPU benchmark database
3DMark Vantage, 3DMark06, 3DMark05, 3DMark03, 3DMark01, Aquamark and Super PI benchmark database
Linux benchmark suite
Open source benchmark programs
DEISA Benchmark Suite
Research frontier of performance and benchmarks
Roy Longbottom’s PC Benchmark Collection (Free) - Source code, pre-compiled versions and results for numerous benchmarks.- Kernels, Synthetic, Component (CPU, Caches, RAM, Graphics, Disk and other I/O, Network), Real/Simulated Real, Burn-in
An OpenSource Schema Evolution Benchmark based on MediaWiki DB history




Retrieved from "http://en.wikipedia.org/wiki/Benchmark_(computing)"
Categories: Computer benchmarks 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Bosanski
Dansk
Deutsch
Español
Français
Galego
Íslenska
Italiano
עברית
Nederlands
日本語
Polski
Português
Русский
Simple English
ไทย
Türkçe









 This page was last modified on 1 April 2009, at 14:36.
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
