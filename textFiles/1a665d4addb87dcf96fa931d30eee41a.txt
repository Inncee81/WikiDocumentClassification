













Nearest neighbor search - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Nearest_neighbor_search";
		var wgTitle = "Nearest neighbor search";
		var wgAction = "view";
		var wgArticleId = "7309022";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 279085044;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Nearest neighbor search

From Wikipedia, the free encyclopedia

Jump to: navigation, search 
Nearest neighbor search (NNS), also known as proximity search, similarity search or closest point search, is an optimization problem for finding closest points in metric spaces. The problem is: given a set S of points in a metric space M and a query point q ∈ M, find the closest point in S to q. In many cases, M is taken to be d-dimensional Euclidean space and distance is measured by Euclidean distance or Manhattan distance.
Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning a residence to the nearest post office.




Contents


1 Applications
2 Methods

2.1 Linear search
2.2 Space partitioning
2.3 Locality sensitive hashing
2.4 Nearest neighbor search in spaces with small intrinsic dimension


3 Variants

3.1 K-nearest neighbor
3.2 Approximate nearest neighbor
3.3 Nearest neighbor distance ratio
3.4 All nearest neighbors


4 See also
5 Notes
6 References
7 External links





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Applications
The nearest neighbor search problem arises in numerous fields of application, including:

Pattern recognition - in particular for optical character recognition
Statistical classification- see k-nearest neighbor algorithm
Computer vision
Databases - e.g. content-based image retrieval
Coding theory - see maximum likelihood decoding
Data compression - see MPEG-2 standard
Recommendation systems
Internet marketing - see contextual advertising and behavioral targeting
DNA sequencing
Spell checking - suggesting correct spelling
Plagiarism detection
Contact searching algorithms in FEA
Similarity scores for predicting career paths of professional athletes.


[edit] Methods
Various solutions to the NNS problem have been proposed. The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the curse of dimensionality states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.

[edit] Linear search
The simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the "best so far". This algorithm, sometimes referred to as the naive approach, works for small databases but quickly becomes intractable as either the size or the dimensionality of the problem becomes large. Linear search has a running time of O(Nd) where N is the cardinality of S and d is the dimensionality of S. There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database.

[edit] Space partitioning
Starting from 1970s branch and bound methodology was applied to the problem. In the case of Euclidean space this approach is known as spatial index or spatial access methods. Several space-partitioning methods have been developed for solving the NNS problem. Perhaps the simplest is the kd-tree, which iteratively bisects the search space into two regions containing half of the points of the parent region. Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. For constant dimension query time, average complexity is O(log N) [1] in the case of randomly distributed points, worst case complexity analyses have been performed [2]. . Alternatively the R-tree data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions.
In case of general metric space branch and bound approach is known under the name of metric trees. Particular examples include VP-tree and Bk-tree.

[edit] Locality sensitive hashing
Locality sensitive hashing (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.

[edit] Nearest neighbor search in spaces with small intrinsic dimension
The cover tree has a theoretical bound that is based on the dataset's doubling constant. The bound on search time is O(c12 log n) where c is the expansion constant of the dataset.

[edit] Variants
There are numerous variants of the NNS problem and the two most well-known are the k-nearest neighbor search and the ε-approximate nearest neighbor search.

[edit] K-nearest neighbor
Main article: K-nearest neighbor algorithm
k-nearest neighbor search identifies the top k nearest neighbors to the query. This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors.

[edit] Approximate nearest neighbor
In some applications it may be acceptable to retrieve a "good guess" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.
Algorithms which support the approximate nearest neighbor search include Best Bin First and the kd-trie.
ε-approximate nearest neighbor search is becoming an increasingly popular tool for fighting the curse of dimensionality.

[edit] Nearest neighbor distance ratio
Nearest neighbor distance ratio do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in CBIR to retrieve pictures through a "query by example" using the similarity between local features. More generally it is involved in several matching problems.

[edit] All nearest neighbors
For some applications (e.g. entropy estimation), we may have N data-points and wish to know which is the nearest neighbor for every one of those N points. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these N queries to produce a more efficient search. As a simple example: when we find the distance from point X to point Y, that also tells us the distance from point Y to point X, so the same calculation can be reused in two different queries.
Given a fixed dimension, a semi-definite positive norm (thereby including every Lp norm), and n points in this space, the m nearest neighbours of every point can be found in O(mnlogn) time.[3]

[edit] See also

K-nearest neighbor algorithm
Nearest-neighbor interpolation
Content-based image retrieval
Locality sensitive hashing
Voronoi diagram


[edit] Notes

^ Andrew Moore. "An introductory tutorial on KD trees". http://www.autonlab.com/autonweb/14665/version/2/part/5/data/moore-tutorial.pdf?branch=main&language=en. 
^ Lee, D. T.; Wong, C. K. (1977), "Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees", Acta Informatica 9 (1): 23–29, doi:10.1007/BF00263763 
^ Vaidya, P. M. (1989), "An O(n logn) Algorithm for the All-Nearest-Neighbors Problem", Discrete and Computational Geometry 4 (1): 101-115, http://www.springerlink.com/content/p4mk2608787r7281/?p=09da9252d36e4a1b8396833710ef08cc&pi=8 


[edit] References

Arya, S., D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu. An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions. Journal of the ACM, vol. 45, no. 6, pp. 891-923
Zezula, P., Amato, G., Dohnal, V., and Batko, M. Similarity Search - The Metric Space Approach. Springer, 2006. ISBN: 0-387-29146-6
Chung-Min Chen and Yibei Ling - A Sampling-Based Estimator for Top-k Query. ICDE 2002: 617-627


[edit] External links

Nearest Neighbors and Similarity Search - a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits.
Metric Spaces Library - An open-source C-based library for metric space indexing by Karina Figueroa, Gonzalo Navarro, Edgar Chávez.
ANN - A Library for Approximate Nearest Neighbor Searching by David M. Mount and Sunil Arya.
STANN - A Simple Threaded Approximate Nearest Neighbor Search Library in C++ by Michael Connor and Piyush Kumar.
MESSIF - Metric Similarity Search Implementation Framework by Michal Batko and David Novak.
OBSearch - Metric Similarity Search (distributed, GPL). Implementation by Arnoldo Muller, developed during Google Summer of Code 2007.
KNNLSB - K Nearest Neighbors Linear Scan Baseline (distributed, GPL). Implementation by Georges Quénot (LIG-CNRS).




Retrieved from "http://en.wikipedia.org/wiki/Nearest_neighbor_search"
Categories: Analysis of algorithms | Approximation algorithms | Classification algorithms | Data mining | Discrete geometry | Geometric algorithms | Image processing | Information retrieval | Machine learning | Numerical analysis | Mathematical optimization | Optimization algorithms | Searching | Search algorithms 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


العربية
Français
日本語
Русский
中文









 This page was last modified on 23 March 2009, at 04:23.
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
