













Supercomputer - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Supercomputer";
		var wgTitle = "Supercomputer";
		var wgAction = "view";
		var wgArticleId = "37153";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 281647797;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Supercomputer

From Wikipedia, the free encyclopedia

Jump to: navigation, search 
For other uses, see Supercomputer (disambiguation).





This article needs additional citations for verification. Please help improve this article by adding reliable references (ideally, using inline citations). Unsourced material may be challenged and removed. (July 2008)






The Cray-2, the world's fastest computer from 1985 to 1989


A supercomputer is a computer that is at the frontline of current processing capacity, particularly speed of calculation. Supercomputers introduced in the 1960s were designed primarily by Seymour Cray at Control Data Corporation (CDC), and led the market into the 1970s until Cray left to form his own company, Cray Research. He then took over the supercomputer market with his new designs, holding the top spot in supercomputing for five years (1985–1990). In the 1980s a large number of smaller competitors entered the market, in parallel to the creation of the minicomputer market a decade earlier, but many of these disappeared in the mid-1990s "supercomputer market crash".
Today, supercomputers are typically one-of-a-kind custom designs produced by "traditional" companies such as Cray, IBM and Hewlett-Packard, who had purchased many of the 1980s companies to gain their experience. As of May 2008[update], the IBM Roadrunner, located at Los Alamos National Laboratory, is the fastest supercomputer in the world.
The term supercomputer itself is rather fluid, and today's supercomputer tends to become tomorrow's ordinary computer. CDC's early machines were simply very fast scalar processors, some ten times the speed of the fastest machines offered by other companies. In the 1970s most supercomputers were dedicated to running a vector processor, and many of the newer players developed their own such processors at a lower price to enter the market. The early and mid-1980s saw machines with a modest number of vector processors working in parallel to become the standard. Typical numbers of processors were in the range of four to sixteen. In the later 1980s and 1990s, attention turned from vector processors to massive parallel processing systems with thousands of "ordinary" CPUs, some being off the shelf units and others being custom designs. Today, parallel designs are based on "off the shelf" server-class microprocessors, such as the PowerPC, Opteron, or Xeon, and most modern supercomputers are now highly-tuned computer clusters using commodity processors combined with custom interconnects.




Contents


1 Common uses
2 Hardware and software design

2.1 Supercomputer challenges, technologies
2.2 Processing techniques
2.3 Operating systems
2.4 Programming
2.5 Software tools


3 Modern supercomputer architecture
4 Special-purpose supercomputers
5 The fastest supercomputers today

5.1 Measuring supercomputer speed
5.2 The Top500 list
5.3 Current fastest supercomputer system
5.4 Quasi-supercomputing


6 Research and development
7 Timeline of supercomputers
8 See also

8.1 Supercomputer Companies / Manufacturer

8.1.1 Supercomputer companies in operation
8.1.2 Defunct supercomputer companies


8.2 General concepts and history


9 Notes
10 External links

10.1 Information resources
10.2 Supercomputing centers, organizations
10.3 Specific machines, general-purpose
10.4 Specific machines, special-purpose







//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Common uses
Supercomputers are used for highly calculation-intensive tasks such as problems involving quantum mechanical physics, weather forecasting, climate research, molecular modeling (computing the structures and properties of chemical compounds, biological macromolecules, polymers, and crystals), physical simulations (such as simulation of airplanes in wind tunnels, simulation of the detonation of nuclear weapons, and research into nuclear fusion), cryptanalysis, and the like. Major universities, military agencies and scientific research laboratories are heavy users.
A particular class of problems, known as Grand Challenge problems, are problems whose full solution requires semi-infinite computing resources.
Relevant here is the distinction between capability computing and capacity computing, as defined by Graham et al. Capability computing is typically thought of as using the maximum computing power to solve a large problem in the shortest amount of time. Often a capability system is able to solve a problem of a size or complexity that no other computer can. Capacity computing in contrast is typically thought of as using efficient cost-effective computing power to solve somewhat large problems or many small problems or to prepare for a run on a capability system.

[edit] Hardware and software design





This section does not cite any references or sources. Please help improve this article by adding citations to reliable sources (ideally, using inline citations). Unsourced material may be challenged and removed. (July 2008)






Processor board of a CRAY YMP vector computer


Supercomputers using custom CPUs traditionally gained their speed over conventional computers through the use of innovative designs that allow them to perform many tasks in parallel, as well as complex detail engineering. They tend to be specialized for certain types of computation, usually numerical calculations, and perform poorly at more general computing tasks. Their memory hierarchy is very carefully designed to ensure the processor is kept fed with data and instructions at all times — in fact, much of the performance difference between slower computers and supercomputers is due to the memory hierarchy. Their I/O systems tend to be designed to support high bandwidth, with latency less of an issue, because supercomputers are not used for transaction processing.
As with all highly parallel systems, Amdahl's law applies, and supercomputer designs devote great effort to eliminating software serialization, and using hardware to address the remaining bottlenecks.

[edit] Supercomputer challenges, technologies

A supercomputer generates large amounts of heat and must be cooled. Cooling most supercomputers is a major HVAC problem.
Information cannot move faster than the speed of light between two parts of a supercomputer. For this reason, a supercomputer that is many metres across must have latencies between its components measured at least in the tens of nanoseconds. Seymour Cray's supercomputer designs attempted to keep cable runs as short as possible for this reason: hence the cylindrical shape of his Cray range of computers. In modern supercomputers built of many conventional CPUs running in parallel, latencies of 1-5 microseconds to send a message between CPUs are typical.
Supercomputers consume and produce massive amounts of data in a very short period of time. According to Ken Batcher, "A supercomputer is a device for turning compute-bound problems into I/O-bound problems." Much work on external storage bandwidth is needed to ensure that this information can be transferred quickly and stored/retrieved correctly.

Technologies developed for supercomputers include:

Vector processing
Liquid cooling
Non-Uniform Memory Access (NUMA)
Striped disks (the first instance of what was later called RAID)
Parallel filesystems


[edit] Processing techniques
Vector processing techniques were first developed for supercomputers and continue to be used in specialist high-performance applications. Vector processing techniques have trickled down to the mass market in DSP architectures and SIMD (Single Instruction Multiple Data) processing instructions for general-purpose computers.
Modern video game consoles in particular use SIMD extensively and this is the basis for some manufacturers' claim that their game machines are themselves supercomputers. Indeed, some graphics cards have the computing power of several TeraFLOPS. The applications to which this power can be applied was limited by the special-purpose nature of early video processing. As video processing has become more sophisticated, Graphics processing units (GPUs) have evolved to become more useful as general-purpose vector processors, and an entire computer science sub-discipline has arisen to exploit this capability: General-Purpose Computing on Graphics Processing Units (GPGPU).

[edit] Operating systems




Supercomputers predominantly run some variant of Linux.


Supercomputer operating systems, today most often variants of Linux, are at least as complex as those for smaller machines. Historically, their user interfaces tended to be less developed, as the OS developers had limited programming resources to spend on non-essential parts of the OS (i.e., parts not directly contributing to the optimal utilization of the machine's hardware). These computers, often priced at millions of dollars, are sold to a very small market and the R&D budget for the OS was often limited. The advent of Unix and Linux allows reuse of conventional desktop software and user interfaces.
Interestingly this has been a continuing trend throughout the supercomputer industry, with former technology leaders such as Silicon Graphics taking a back seat to such companies as AMD and NVIDIA, who have been able to produce cheap, feature-rich, high-performance, and innovative products due to the vast number of consumers driving their R&D.
Until the early-to-mid-1980s, supercomputers usually sacrificed instruction set compatibility and code portability for performance (processing and memory access speed). For the most part, supercomputers to this time (unlike high-end mainframes) had vastly different operating systems. The Cray-1 alone had at least six different proprietary OSs largely unknown to the general computing community. Similarly different and incompatible vectorizing and parallelizing compilers for Fortran existed. This trend would have continued with the ETA-10 were it not for the initial instruction set compatibility between the Cray-1 and the Cray X-MP, and the adoption of UNIX operating system variants (such as Cray's Unicos) and today's Linux.
In the future, the highest performance systems are likely to use a variant of Linux but with incompatible system-unique features (especially for the highest-end systems at secure facilities).[citation needed]

[edit] Programming
The parallel architectures of supercomputers often dictate the use of special programming techniques to exploit their speed. The base language of supercomputer code is generally Fortran or C, using special libraries to share data between nodes. Most commonly, environments such as PVM and MPI for loosely connected clusters and OpenMP for tightly coordinated shared memory machines are used. Significant effort is required to optimize a problem for the interconnect characteristics of the machine it will be run on; the aim is to prevent any of the CPU's from wasting time waiting on data from other nodes.

[edit] Software tools
Software tools for distributed processing include standard APIs such as MPI and PVM, VTL and open source-based software solutions such as Beowulf, WareWulf and openMosix which facilitate the creation of a supercomputer from a collection of ordinary workstations or servers. Technology like ZeroConf (Rendezvous/Bonjour) can be used to create ad hoc computer clusters for specialized software such as Apple's Shake compositing application. An easy programming language for supercomputers remains an open research topic in computer science. Several utilities that would once have cost several thousands of dollars are now completely free thanks to the open source community which often creates disruptive technology in this arena.

[edit] Modern supercomputer architecture





This section does not cite any references or sources. Please help improve this article by adding citations to reliable sources (ideally, using inline citations). Unsourced material may be challenged and removed. (July 2008)






IBM Roadrunner - LANL






The CPU Architecture Share of Top500 Rankings between 1998 and 2007: x86 family includes x86-64.


As of November 2006, the top ten supercomputers on the Top500 list (and indeed the bulk of the remainder of the list) have the same top-level architecture. Each of them is a cluster of MIMD multiprocessors, each processor of which is SIMD. The supercomputers vary radically with respect to the number of multiprocessors per cluster, the number of processors per multiprocessor, and the number of simultaneous instructions per SIMD processor. Within this hierarchy we have:

A computer cluster is a collection of computers that are highly interconnected via a high-speed network or switching fabric. Each computer runs under a separate instance of an Operating System (OS).
A multiprocessing computer is a computer, operating under a single OS and using more than one CPU, where the application-level software is indifferent to the number of processors. The processors share tasks using Symmetric multiprocessing (SMP) and Non-Uniform Memory Access (NUMA).
A SIMD processor executes the same instruction on more than one set of data at the same time. The processor could be a general purpose commodity processor or special-purpose vector processor. It could also be high performance processor or a low power processor. As of 2007, the processor executes several SIMD instructions per nanosecond.

As of November 2008 the fastest machine is IBM Roadrunner. This machine is a cluster of 3240 computers, each with 40 processing cores. By contrast, Columbia is a cluster of 20 machines, each with 512 processors, each of which processes two data streams concurrently.
As of February 2009, IBM has announced work on "Sequioa" which will be a 20 petaflops supercomputer. This will be equivalent to 2 million laptops (whereas Roadrunner is comparable to a mere 100,000 laptops). It is slated for deployment in 2011. [1]
Moore's Law and economies of scale are the dominant factors in supercomputer design: a single modern desktop PC is now more powerful than a ten-year old supercomputer, and the design concepts that allowed past supercomputers to out-perform contemporaneous desktop machines have now been incorporated into commodity PCs. Furthermore, the costs of chip development and production make it uneconomical to design custom chips for a small run and favor mass-produced chips that have enough demand to recoup the cost of production. A current model quad-core Xeon workstation running at 2.66 GHz will outperform a multimillion dollar Cray C90 supercomputer used in the early 1990s; most workloads requiring such a supercomputer in the 1990s can now be done on workstations costing less than 4,000 US dollars. Supercomputing is taking a step of increasing density allowing for Desktop Supercomputers to become available, offering the compute power that in 1998 required a large room to require less than a Desktop footprint. i.e. Supermicro Blade server centre that can house 160 Cores in only 21.65”W x 34.65”D x 30.64”H (Up to 40 CPU's, 160 Processor Cores, and 640GB memory for 4-way version SuperBlade®)
Additionally, many problems carried out by supercomputers are particularly suitable for parallelization (in essence, splitting up into smaller parts to be worked on simultaneously) and, particularly, fairly coarse-grained parallelization that limits the amount of information that needs to be transferred between independent processing units. For this reason, traditional supercomputers can be replaced, for many applications, by "clusters" of computers of standard design which can be programmed to act as one large computer.

[edit] Special-purpose supercomputers





This section does not cite any references or sources. Please help improve this article by adding citations to reliable sources (ideally, using inline citations). Unsourced material may be challenged and removed. (July 2008)


Special-purpose supercomputers are high-performance computing devices with a hardware architecture dedicated to a single problem. This allows the use of specially programmed FPGA chips or even custom VLSI chips, allowing higher price/performance ratios by sacrificing generality. They are used for applications such as astrophysics computation and brute-force codebreaking. Historically a new special-purpose supercomputer has occasionally been faster than the world's fastest general-purpose supercomputer, by some measure. For example, GRAPE-6 was faster than the Earth Simulator in 2002 for a particular special set of problems.
Examples of special-purpose supercomputers:

Belle, Deep Blue, and Hydra, for playing chess
Reconfigurable computing machines or parts of machines
GRAPE, for astrophysics and molecular dynamics
Deep Crack, for breaking the DES cipher
MDGRAPE-3, for protein structure computation


[edit] The fastest supercomputers today

[edit] Measuring supercomputer speed
The speed of a supercomputer is generally measured in "FLOPS" (FLoating Point Operations Per Second), commonly used with an SI prefix such as tera-, combined into the shorthand "TFLOPS" (1012 FLOPS, pronounced teraflops), or peta-, combined into the shorthand "PFLOPS" (1015 FLOPS, pronounced petaflops.) This measurement is based on a particular benchmark which does LU decomposition of a large matrix. This mimics a class of real-world problems, but is significantly easier to compute than a majority of actual real-world problems.
"Petascale" supercomputers that can process 1000 trillion FLOPS. Exascale is computing performance in the exaflops range. An exaflop is one million teraflops.

[edit] The Top500 list
Main article: TOP500
Since 1993, the fastest supercomputers have been ranked on the Top500 list according to their LINPACK benchmark results. The list does not claim to be unbiased or definitive, but it is a widely cited current definition of the "fastest" supercomputer available at any given time.

[edit] Current fastest supercomputer system




A Blue Gene/P node card


On June 8, 2008, the Cell/AMD Opteron-based IBM Roadrunner at the Los Alamos National Laboratory (LANL) was announced as the fastest operational supercomputer, with a sustained processing rate of 1.026 PFLOPS.[2] The Roadrunner hardware and software was then optimized and the benchmark was re-run and submitted for the November 2008 TOP500 with an Rmax of 1.105 PFLOPS, barely surviving a challenge from the Cray XT5 Jaguar to remain the fastest computer on the "official" list.[3]

[edit] Quasi-supercomputing
Some types of large-scale distributed computing for embarrassingly parallel problems take the clustered supercomputing concept to an extreme.
One such example is the BOINC platform, a host for a number of distributed computing projects. As of February 2009[update], BOINC recorded a processing power of over 1.7 petaflops through over 530,000 active computers on the network.[4] The largest project, SETI@home, reported processing power of over 508 teraflops through almost 317,000 active computers.[5]
Another distributed computing project, Folding@home, reported over 4.5 petaflops of processing power as of December 2008. A little over 1.5 petaflops of this processing power is contributed by clients running on PlayStation 3 systems and another 2.6 petaflops is contributed by their newly released GPU2 client.[6]
As of May 2008[update], GIMPS's distributed Mersenne Prime search achieves currently 29 teraflops.[citation needed]
Also a “quasi-supercomputer” is Google's search engine system with estimated total processing power of between 126 and 316 teraflops, as of April 2004.[7] In June 2006 the New York Times estimated that the Googleplex and its server farms contain 450,000 servers.[8] According to recent estimations, the processing power of Google's cluster might reach from 20 to 100 petaflops.[9]
The PlayStation 3 Gravity Grid uses a network of 16 machines, and exploits the Cell processor for the intended application which is binary black hole coalescence using perturbation theory.[10][11] The Cell processor has a main CPU and 6 floating-point vector processors, giving the machine a net of 16 general-purpose machines and 96 vector processors. The machine has a one-time cost of $9,000 to build and is adequate for black-hole simulations which would otherwise cost $6,000 per run on a conventional supercomputer. The black hole calculations are not memory-intensive and are highly localizable, and so are well-suited to this architecture.

[edit] Research and development




Futurist Ray Kurzweil's projected supercomputer processing power


IBM is developing the Cyclops64 architecture, intended to create a "supercomputer on a chip".
Other PFLOPS projects include one by Narendra Karmarkar in India,[12] a CDAC effort targeted for 2010,[13] and the Blue Waters Petascale Computing System funded by the NSF ($200 million) that is being built by the NCSA at the University of Illinois at Urbana-Champaign (slated to be completed by 2011).[14]
In May 2008 a collaboration was announced between NASA, SGI and Intel to build a 1 petaflops computer, Pleiades, in 2009, scaling up to 10 PFLOPs by 2012.[15]
Given the current speed of progress, supercomputers are projected to reach 1 exaflops (1018) in 2019.[16] Futurist Ray Kurzweil expects supercomputers capable of human brain neural simulations, for which according to Kurzweil 10 exaflops (1019) would be required, in 2025.
Erik P. DeBenedictis of Sandia National Laboratories theorizes that a zettaflops (1021) computer is required to accomplish full weather modeling, which could cover a two week time span accurately.[17] Such systems might be built around 2030.[18]


[edit] Timeline of supercomputers
This is a list of the record-holders for fastest general-purpose supercomputer in the world, and the year each one set the record. For entries prior to 1993, this list refers to various sources[19][citation needed]. From 1993 to present, the list reflects the Top500 listing[20], and the "Peak speed" is given as the "Rmax" rating.


Year
Supercomputer
Peak speed
(Rmax)
Location


1942
Atanasoff–Berry Computer (ABC)
30 OPS
Iowa State University, Ames, Iowa, USA


TRE Heath Robinson
200 OPS
Bletchley Park, Bletchley, UK


1944
Flowers Colossus
5 kOPS
Post Office Research Station, Dollis Hill, UK


1946
 
UPenn ENIAC
(before 1948+ modifications)
100 kOPS
Department of War
Aberdeen Proving Ground, Maryland, USA
 


1954
IBM NORC
67 kOPS
Department of Defense
U.S. Naval Proving Ground, Dahlgren, Virginia, USA


1956
MIT TX-0
83 kOPS
Massachusetts Inst. of Technology, Lexington, Massachusetts, USA


1958
IBM AN/FSQ-7
400 kOPS
25 U.S. Air Force sites across the continental USA and 1 site in Canada (52 computers)


1960
UNIVAC LARC
250 kFLOPS
Atomic Energy Commission (AEC)
Lawrence Livermore National Laboratory, California, USA


1961
IBM 7030 "Stretch"
1.2 MFLOPS
AEC-Los Alamos National Laboratory, New Mexico, USA


1964
CDC 6600
3 MFLOPS
AEC-Lawrence Livermore National Laboratory, California, USA


1969
CDC 7600
36 MFLOPS


1974
CDC STAR-100
100 MFLOPS


1975
Burroughs ILLIAC IV
150 MFLOPS
NASA Ames Research Center, California, USA


1976
Cray-1
250 MFLOPS
Energy Research and Development Administration (ERDA)
Los Alamos National Laboratory, New Mexico, USA (80+ sold worldwide)


1981
CDC Cyber 205
400 MFLOPS
(numerous sites worldwide)


1983
Cray X-MP/4
941 MFLOPS
U.S. Department of Energy (DoE)
Los Alamos National Laboratory; Lawrence Livermore National Laboratory; Battelle; Boeing


1984
M-13
2.4 GFLOPS
Scientific Research Institute of Computer Complexes, Moscow, USSR


1985
Cray-2/8
3.9 GFLOPS
DoE-Lawrence Livermore National Laboratory, California, USA


1989
ETA10-G/8
10.3 GFLOPS
Florida State University, Florida, USA


1990
NEC SX-3/44R
23.2 GFLOPS
NEC Fuchu Plant, Fuchu, Japan


1993
Thinking Machines CM-5/1024
65.5 GFLOPS
DoE-Los Alamos National Laboratory; National Security Agency


Fujitsu Numerical Wind Tunnel
124.50 GFLOPS
National Aerospace Laboratory, Tokyo, Japan


Intel Paragon XP/S 140
143.40 GFLOPS
DoE-Sandia National Laboratories, New Mexico, USA


1994
Fujitsu Numerical Wind Tunnel
170.40 GFLOPS
National Aerospace Laboratory, Tokyo, Japan


1996
Hitachi SR2201/1024
220.4 GFLOPS
University of Tokyo, Japan


Hitachi/Tsukuba CP-PACS/2048
368.2 GFLOPS
Center for Computational Physics, University of Tsukuba, Tsukuba, Japan


1997
Intel ASCI Red/9152
1.338 TFLOPS
DoE-Sandia National Laboratories, New Mexico, USA


1999
Intel ASCI Red/9632
2.3796 TFLOPS


2000
IBM ASCI White
7.226 TFLOPS
DoE-Lawrence Livermore National Laboratory, California, USA


2002
NEC Earth Simulator
35.86 TFLOPS
Earth Simulator Center, Yokohama, Japan


2004
IBM Blue Gene/L
70.72 TFLOPS
DoE/IBM Rochester, Minnesota, USA


2005
136.8 TFLOPS
DoE/U.S. National Nuclear Security Administration,
Lawrence Livermore National Laboratory, California, USA


280.6 TFLOPS


2007
478.2 TFLOPS


2008
IBM Roadrunner
1.026 PFLOPS
DoE-Los Alamos National Laboratory, New Mexico, USA


1.105 PFLOPS



[edit] See also




Computer Science portal







Electronics portal




[edit] Supercomputer Companies / Manufacturer

[edit] Supercomputer companies in operation
These companies make supercomputer hardware and/or software, either as their sole activity, or as one of several activities.


Advanced Micro Devices, Inc.
Cray Inc.
Dell
Dawning
Fujitsu
Groupe Bull
Hitachi
HP
IBM
Intel Corporation
Lenovo
Microsoft
nCUBE
NEC Corporation
NVIDIA Corporation
Quadrics
SiCortex
SGI
Sun Microsystems
Supercomputing Systems
Galactic Computing
Supermicro Computer SMCI



[edit] Defunct supercomputer companies
These companies have either folded, or no longer operate in the supercomputer market.


Control Data Corporation (CDC)
Convex Computer
Kendall Square Research
MasPar Computer Corporation
Meiko Scientific
Sequent Computer Systems
Supercomputer Systems, Inc., Eau Claire, Wisconsin, S. Chen
Supercomputer Systems, Inc., San Diego, California
Thinking Machines



[edit] General concepts and history


Beowulf cluster
Computational Science and Engineering
Distributed computing
Flash mob computer
Grid computing
High-performance computing (HPC)
History of computing hardware
Metacomputing
MOSIX
Parallel computing
Symmetric multiprocessing
Quantum computer
World Community Grid



[edit] Notes


^ http://www.networkworld.com/news/2009/020409-ibm-to-build-new-monster.html?page=1
^ "June 2008". cnet.com. http://news.cnet.com/Military-supercomputer-sets-record/2100-1010_3-6241145.html?tag=nefd.top. Retrieved on 2008-06-09. 
^ "Jaguar Chases Roadrunner, but Can’t Grab Top Spot on Latest List of World’s TOP500 Supercomputers". TOP500. 2008-11-14. http://www.top500.org/lists/2008/11/press-release. Retrieved on 2008-11-18.  }}
^ "BOINCstats: BOINC Combined". BOINC. http://www.boincstats.com/stats/project_graph.php?pr=bo. Retrieved on 2008-12-22. 
^ "BOINCstats: SETI@Home". BOINC. http://www.boincstats.com/stats/project_graph.php?pr=sah. Retrieved on 2008-12-22. 
^ "Folding@home: OS Statistics". Stanford University. http://fah-web.stanford.edu/cgi-bin/main.py?qtype=osstats. 
^ How many Google machines, April 30, 2004
^ Markoff, John; Hensell, Saul (June 14, 2006). "Hiding in Plain Sight, Google Seeks More Power". New York Times. http://www.nytimes.com/2006/06/14/technology/14search.html. Retrieved on 2008-03-16. 
^ Google Surpasses Supercomputer Community, Unnoticed?, May 20, 2008.
^ "PlayStation 3 tackles black hole vibrations", by Tariq Malik, January 28, 2009, MSNBC
^ PlayStation3 Gravity Grid
^ Athley, Gouri Agtey; Rajeshwari Adappa (30 October, 2006). ""Tatas get Karmakar to make super comp"". The Economic Times. http://economictimes.indiatimes.com/articleshow/msid-225517,curpg-2.cms. Retrieved on 2008-03-16. 
^ C-DAC's Param programme sets to touch 10 teraflops by late 2007 and a petaflops by 2010.[dead link]
^ ""National Science Board Approves Funds for Petascale Computing Systems"". U.S. National Science Foundation. August 10, 2007. http://www.nsf.gov/news/news_summ.jsp?cntn_id=109850. Retrieved on 2008-03-16. 
^ "NASA collaborates with Intel and SGI on forthcoming petaflops super computers". Heise online. 2008-05-09. http://www.heise.de/english/newsticker/news/107683. 
^ Thibodeau, Patrick (2008-06-10). "IBM breaks petaflop barrier". InfoWorld. http://www.infoworld.com/article/08/06/10/IBM_breaks_petaflop_barrier_1.html. 
^ DeBenedictis, Erik P. (2005). "Reversible logic for supercomputing". Proceedings of the 2nd conference on Computing frontiers. pp. 391–402. ISBN 1595930191. 
^ "IDF: Intel says Moore's Law holds until 2029". Heise Online. 2008-04-04. http://www.heise.de/english/newsticker/news/106017. 
^ CDC timeline at Computer History Museum
^ Directory page for Top500 lists



[edit] External links





This article's external links may not follow Wikipedia's content policies or guidelines. Please improve this article by removing excessive or inappropriate external links.



[edit] Information resources

Military Supercomputer
TOP500 Supercomputer list
Green500 Supercomputer list by efficiency
LinuxHPC.org Linux High Performance Computing and Clustering Portal
WinHPC.org Windows High Performance Computing and Clustering Portal
Cluster Resources
Cluster Builder
Centre for Development of Advanced Computing (CDAC)
Microsoft Windows Compute Cluster Server (CCS)
Infiscale Cluster Portal - Free GPL HPC Resources
Supercomputing Online HPC, Networking & Storage Professionals
Degree Project about best alternatives to implement HPC Cluster


[edit] Supercomputing centers, organizations
Organizations

DEISA Distributed European Infrastructure for Supercomputing Applications, a facility integrating eleven European supercomputing centers.
NAREGI Japanese NAtional REsearch Grid Initiative involving several supercomputer centers
PHASE Editorial CommitteeAIST Parallel and High Performance Applicational Software Exchange
TeraGrid, a national facility integrating nine US supercomputing centers

Centers

ARSC Arctic Region Supercomputing Center at University of Alaska Fairbanks
BSC Barcelona Supercomputing Center - Spanish national supercomputing facility and R&D center
CESCA Supercomputing Centre of Catalonia - Centre de Supercomputacio de Catalunya
CESGA Galicia Supercomputing Center - Centro de Supercomputación de Galicia
CeSViMa Supercomputing and Visualization Center of Madrid
CINECA CINECA Interuniversity Consortium, Italy
CINES Centre Informatique National de l'Enseignement Superieur, France
CSAR UK national supercomputer service operated by Manchester Computing
EPCC Edinburgh Parallel Computing Centre. Based in the University of Edinburgh.
GSIC Global Scientific Information and Computing Center at the Tokyo Institute of Technology
HECToR UK national supercomputer service provided by a consortium of EPCC, Cray and Numerical Algorithms Group (NAG)
HPCx UK national supercomputer service operated by EPCC and Daresbury Lab
IRB
Minnesota Supercomputer Institute (Formerly Minnesota Supercomputer Center) operated by University of Minnesota
NASA Advanced Supercomputing facility
National Center for High Performance Computing, operated by Technical University of Istanbul
National Center for Atmospheric Research (NCAR)
National Center for Supercomputing Applications (NCSA)
National Energy Research Scientific Computing Center (NERSC)
Ohio Supercomputer Center (OSC)
Pittsburgh Supercomputing Center operated by University of Pittsburgh and Carnegie Mellon University.
Research Computing Services (web site) at the University of Manchester.
San Diego Supercomputer Center (SDSC)
SARA (Stichting Academisch Rekencentrum Amsterdam), Amsterdam, The Netherlands
System X at Virginia Tech
Texas Advanced Computing Center (TACC)
WestGrid
TCHPC Trinity Centre for High Performance Computing. Based in the University of Dublin.
DCSC Danish Centre for Scientific Computing. Based at the University of Copenhagen.
PSNC (Poznan Supercomputing and Networking Center), Poznan, Poland
NSC National Supercomputer Centre in Sweden at Linköping University, Sweden


[edit] Specific machines, general-purpose

Linux NetworX press release: Linux NetworX to build "largest" Linux supercomputer
ASCI White press release
MCR @ LLNL Linux NetworX Supermicro based Supercomputer "3rd largest supercomputer in 2004"
Article about Japanese "Earth Simulator" computer
"Earth Simulator" website (in English)
NEC high-performance computing information
Superconducting Supercomputer
Blue Waters Petascale Computing System
Playstation 3 Gravity Grid


[edit] Specific machines, special-purpose

Papers on the GRAPE special-purpose computer
More special-purpose supercomputer information
Information about the APEmille special-purpose computer
Information about the apeNEXT special-purpose computer
Information about the QCDOC project, machines









v • d • e

Parallel computing topics





General

High-performance computing  · Cluster computing  · Distributed computing  · Grid computing






Parallelism (levels)

Bit · Instruction  · Data  · Task






Threads

Superthreading · Hyperthreading






Theory

Amdahl's law  · Gustafson's law  · Cost efficiency · Karp-Flatt metric  · slowdown  · speedup






Elements

Process · Thread · Fiber · PRAM






Coordination

Multiprocessing · Multithreading · Memory coherency · Cache coherency · Barrier · Synchronization  · Application checkpointing






Programming

Models (Implicit parallelism · Explicit parallelism  · Concurrency)  · Flynn's taxonomy (SISD • SIMD • MISD • MIMD)






Hardware

Multiprocessing (Symmetric  · Asymmetric)  · Memory (NUMA  · COMA  · distributed  · shared  · distributed shared)  · SMT
MPP  · Superscalar  · Vector processor  · Supercomputer · Beowulf






APIs

POSIX Threads · OpenMP · MPI · UPC · Intel Threading Building Blocks · Boost.Thread · Global Arrays · Charm++ · Cilk






Problems

Embarrassingly parallel · Grand Challenge · Software lockout













v • d • e

Computer sizes





Larger

Super · Minisuper · Mainframe · Mini · Supermini · Server









Micro

Personal · Workstation · Home · Desktop · SFF (Nettop) · Plug






Mobile





Portable / Desktop replacement computer · Laptop · Subnotebook · Tablet (Ultra-Mobile PC) · Portable / Mobile data terminal · Electronic organizer · E-book reader · Pocket computer · Handheld game console · Wearable computer







PDAs / IAs


Handheld PC · Pocket PC · Smartphone · PMPs · DAPs







Calculators


Graphing









Other

Single-board computer · Wireless sensor network · Microcontroller · Smartdust · Nanocomputer









Retrieved from "http://en.wikipedia.org/wiki/Supercomputer"
Categories: Cluster computing | Parallel computing | Concurrent computing | SupercomputersHidden categories: All articles with dead external links | Articles with dead external links since March 2008 | Articles needing additional references from July 2008 | Articles containing potentially dated statements from May 2008 | All articles containing potentially dated statements | All articles with unsourced statements | Articles with unsourced statements since December 2008 | Articles containing potentially dated statements from February 2009 | Articles with unsourced statements since May 2008 | Articles with unsourced statements since February 2007 | Wikipedia external links cleanup 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


العربية
Български
Català
Česky
Dansk
Deutsch
Ελληνικά
Español
فارسی
Français
한국어
Հայերեն
Bahasa Indonesia
Italiano
עברית
ქართული
ລາວ
Lietuvių
Lumbaart
Malagasy
मराठी
Bahasa Melayu
Nederlands
日本語
‪Norsk (bokmål)‬
Polski
Português
Română
Русский
Simple English
Slovenčina
Slovenščina
Suomi
Svenska
ไทย
Tiếng Việt
Türkçe
Українська
粵語
中文









 This page was last modified on 4 April 2009, at 05:54.
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
