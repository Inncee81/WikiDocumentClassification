













Amdahl's law - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Amdahl\'s_law";
		var wgTitle = "Amdahl\'s law";
		var wgAction = "view";
		var wgArticleId = "2323";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 281841275;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Amdahl's law

From Wikipedia, the free encyclopedia

Jump to: navigation, search 




The speedup of a program using multiple processors in parallel computing is limited by the sequential fraction of the program. For example, if 95% of the program can be parallelized, the theoretical maximum speedup using parallel computing would be 20x as shown in the diagram, no matter how many processors are used.


Amdahl's law, also known as Amdahl's argument,[1] is named after computer architect Gene Amdahl, and is used to find the maximum expected improvement to an overall system when only part of the system is improved. It is often used in parallel computing to predict the theoretical maximum speedup using multiple processors.
The speedup of a program using multiple processors in parallel computing is limited by the time needed for the sequential fraction of the program. For example, if a program needs 20 hours using a single processor core, and a particular portion of 1 hour cannot be parallelized, while the remaining promising portion of 19 hours (95%) can be parallelized, then regardless of how many processors we devote to a parallelized execution of this program, the minimal execution time cannot be less than that critical 1 hour. Hence the speed up is limited up to 20x, as shown in the diagram on the right.




Contents


1 Description
2 Parallelization
3 Relation to law of diminishing returns
4 Speedup in a sequential program
5 See also
6 Notes
7 References
8 External links





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Description
Amdahl's law is a model for the relationship between the expected speedup of parallelized implementations of an algorithm relative to the serial algorithm, under the assumption that the problem size remains the same when parallelized. For example, if for a given problem size a parallelized implementation of an algorithm can run 12% of the algorithm's operations arbitrarily quick (while the remaining 88% of the operations are not parallelizable), Amdahl's law states that the maximum speedup of the parallelized version is 1/(1 - 0.12) = 1.136 times faster than the non-parallelized implementation.
More technically, the law is concerned with the speedup achievable from an improvement to a computation that affects a proportion P of that computation where the improvement has a speedup of S. (For example, if an improvement can speed up 30% of the computation, P will be 0.3; if the improvement makes the portion affected twice as fast, S will be 2.) Amdahl's law states that the overall speedup of applying the improvement will be

.

To see how this formula was derived, assume that the running time of the old computation was 1, for some unit of time. The running time of the new computation will be the length of time the unimproved fraction takes, (which is 1 − P), plus the length of time the improved fraction takes. The length of time for the improved part of the computation is the length of the improved part's former running time divided by the speedup, making the length of time of the improved part P/S. The final speedup is computed by dividing the old running time by the new running time, which is what the above formula does.
Here's another example. We are given a task which is split up into four parts: P1 = 11%, P2 = 18%, P3 = 23%, P4 = 48%, which add up to 100%. Then we say P1 is not sped up, so S1 = 1 or 100%, P2 is sped up 5x, so S2 = 500%, P3 is sped up 20x, so S3 = 2000%, and P4 is sped up 1.6x, so S4 = 160%. By using the formula P1/S1 + P2/S2 + P3/S3 + P4/S4, we find the running time is

0.11/1 + 0.18/5 + 0.23/20 + 0.48/1.6 = 0.4575

or a little less than ½ the original running time which we know is 1. Therefore the overall speed boost is 1/0.4575 = 2.186 or a little more than double the original speed using the formula (P1/S1 + P2/S2 + P3/S3 + P4/S4)−1. Notice how the 20x and 5x speedup don't have much effect on the overall speed boost and running time when 11% is not sped up, and 48% is sped up by 1.6x.

[edit] Parallelization
In the case of parallelization, Amdahl's law states that if P is the proportion of a program that can be made parallel (i.e. benefit from parallelization), and (1 − P) is the proportion that cannot be parallelized (remains serial), then the maximum speedup that can be achieved by using N processors is



In the limit, as N tends to infinity, the maximum speedup tends to 1 / (1-P). In practice, performance/price falls rapidly as N is increased once there is even a small component of (1 − P).
As an example, if P is 90%, then (1 − P) is 10%, and the problem can be speed up by a maximum of a factor of 10, no matter how large the value of N used. For this reason, parallel computing is only useful for either small numbers of processors, or problems with very high values of P: so-called embarrassingly parallel problems. A great part of the craft of parallel programming consists of attempting to reduce (1-P) to the smallest possible value.

[edit] Relation to law of diminishing returns
Amdahl's law is often conflated with the law of diminishing returns, whereas only a special case of applying Amdahl's law demonstrates 'law of diminishing returns'. If one picks optimally (in terms of the achieved speed-up) what to improve, then one will see monotonically decreasing improvements as one improves. If, however, one picks non-optimally, after improving a sub-optimal component and moving on to improve a more optimal component, one can see an increase in return. Consider, for instance, the illustration. If one picks to work on B then A, one finds an increase in return. If, instead, one works on improving A then B, one will find a diminishing return. Thus, strictly speaking, only one (optimal case) can appropriately be said to demonstrate the 'law of diminishing returns'. Note that it is often rational to improve a system in an order that is "non-optimal" in this sense, given that some improvements are more difficult or consuming of development time than others.
Amdahl's law does represent the law of diminishing returns if you are considering what sort of return you get by adding more processors to a machine, if you are running a fixed-size computation that will use all available processors to their capacity. Each new processor you add to the system will add less usable power than the previous one. Each time you double the number of processors the speedup ratio will diminish, as the total throughput heads toward the limit of



This analysis neglects other potential bottlenecks such as memory bandwidth and I/O bandwidth, if they do not scale with the number of processors; however, taking into account such bottlenecks would tend to further demonstrate the diminishing returns of only adding processors.

[edit] Speedup in a sequential program




Assume that a task has two independent parts, A and B. B takes roughly 25% of the time of the whole computation. By working very hard, one may be able to make this part 5 times faster, but this only reduces the time for the whole computation by a little. In contrast, one may need to perform less work to make part A be twice as fast. This will make the computation much faster than by optimizing part B, even though B got a bigger speed-up, (5x versus 2x).


The maximum speedup in an improved sequential program, where some part was sped up by p times is

Max. Speedup 

where f (0.0 < f < 1.0) is the fraction of time (before the improvement) spent in the part that was not improved. For example,

If part B (blue) is made five times faster, p = 5.0, tn (red) = 3 seconds, ti (blue) = 1 second and

f = tn / (tn + ti) = 0.75
Max. Speedup 


If part A (red) is made to run twice as fast, p = 2.0, tn (blue) = 1 second, ti (red) = 3 seconds and

f = tn / (tn + ti) = 0.25
Max. Speedup  (better!)



Therefore, making A twice as fast is better than making B five times faster.

Improving part A by a factor of two will result in a +60% increase in overall program speed.
However, improving part B by a factor of 5 (which presumably requires more effort) will only achieve an overall speedup of +25%.


[edit] See also

Speedup
Amdahl Corporation
Ninety-ninety rule
Gustafson's Law
Karp-Flatt Metric
Brooks's law
Moore's Law


[edit] Notes


^ Rodgers 85, p.226



[edit] References

Gene Amdahl, "[1] Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities", AFIPS Conference Proceedings, (30), pp. 483-485, 1967. Note: Gene Amdahl has approved the use of his complete text in the Usenet comp.sys.super news group FAQ which goes out on the 20th of each month.
Rodgers, David P. (1985) Improvements in multiprocessor system design ACM SIGARCH Computer Architecture News archive Volume 13, Issue 3 (June 1985) table of contents Special Issue: Proceedings of the 12th annual International Symposium on Computer Architecture (ISCA '85) Pages: 225 - 231 Year of Publication: 1985 ISSN:0163-5964. Also published in International Symposium on Computer Architecture, Proceedings of the 12th annual international symposium on Computer architecture, 1985, Boston, Massachusetts, United States


[edit] External links



Wikimedia Commons has media related to: Amdahl's law 



Gene Amdahl. Oral history interview. Charles Babbage Institute, University of Minnesota, Minneapolis.
Reevaluating Amdahl's Law
Reevaluating Amdahl's Law and Gustafson's Law
A simple interactive Amdahl's Law calculator
"Amdahl's Law" by Joel F. Klein, Wolfram Demonstrations Project, 2007.
Amdahl's Law in the Multicore Era
Blog Post: "What the $#@! is Parallelism, Anyhow?"








v • d • e

Parallel computing topics





General

High-performance computing  · Cluster computing  · Distributed computing  · Grid computing






Parallelism (levels)

Bit · Instruction  · Data  · Task






Threads

Superthreading · Hyperthreading






Theory

Amdahl's law  · Gustafson's law  · Cost efficiency · Karp-Flatt metric  · slowdown  · speedup






Elements

Process · Thread · Fiber · PRAM






Coordination

Multiprocessing · Multithreading · Memory coherency · Cache coherency · Barrier · Synchronization  · Application checkpointing






Programming

Models (Implicit parallelism · Explicit parallelism  · Concurrency)  · Flynn's taxonomy (SISD • SIMD • MISD • MIMD)






Hardware

Multiprocessing (Symmetric  · Asymmetric)  · Memory (NUMA  · COMA  · distributed  · shared  · distributed shared)  · SMT
MPP  · Superscalar  · Vector processor  · Supercomputer · Beowulf






APIs

POSIX Threads · OpenMP · MPI · UPC · Intel Threading Building Blocks · Boost.Thread · Global Arrays · Charm++ · Cilk






Problems

Embarrassingly parallel · Grand Challenge · Software lockout









Retrieved from "http://en.wikipedia.org/wiki/Amdahl%27s_law"
Categories: Parallel computing | Rules of thumb 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Català
Deutsch
Español
Français
한국어
Hrvatski
Bahasa Indonesia
Italiano
עברית
日本語
Polski
Português
Русский
Tiếng Việt
Türkçe
中文









 This page was last modified on 5 April 2009, at 04:54 (UTC).
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
