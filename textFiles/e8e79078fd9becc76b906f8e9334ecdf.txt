













Principle of maximum entropy - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Principle_of_maximum_entropy";
		var wgTitle = "Principle of maximum entropy";
		var wgAction = "view";
		var wgArticleId = "201718";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 285189198;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Principle of maximum entropy

From Wikipedia, the free encyclopedia

Jump to: navigation, search 





This article includes a list of references or external links, but its sources remain unclear because it lacks inline citations. Please improve this article by introducing more precise citations where appropriate. (September 2008)


Not to be confused with the law of increasing entropy.
The principle of maximum entropy is a postulate about a universal feature of any probability assignment on a given set of propositions (events, hypotheses, indices, etc.). Let some testable information about a probability distribution function be given. Consider the set of all trial probability distributions that encode this information. Then, the probability distribution that maximizes the information entropy is the true probability distribution with respect to the testable information prescribed.




Contents


1 History
2 Overview
3 Testable information
4 General solution for the maximum entropy distribution with linear constraints

4.1 Discrete case
4.2 Continuous case
4.3 Examples


5 Justifications for the principle of maximum entropy

5.1 Information entropy as a measure of 'uninformativeness'
5.2 The Wallis derivation
5.3 Compatibility with Bayes Rule


6 See also
7 References
8 External links





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] History
The principle was first expounded by E.T. Jaynes in his seminal papers of 1957 where he emphasized a natural correspondence between statistical mechanics and information theory. In particular, Jaynes offered a new and very general rationale why the Gibbsian method of statistical mechanics works. He suggested that the entropy in statistical mechanics, and the information entropy in information theory, are principally the same thing. Consequently, statistical mechanics should be seen just as a particular application of a general tool of logical inference and information theory.

[edit] Overview
In most practical cases, the testable information is given by a set of conserved quantities (average values of some moment functions), associated with the probability distribution in question. In this way the maximum entropy principle is most often used in statistical thermodynamics. Another possibility is to prescribe some symmetries of the probability distribution. An equivalence between the conserved quantities and corresponding symmetry groups implies the same level of equivalence for both these two ways of specifying the testable information in the maximum entropy method.
The maximum entropy principle is also needed to guarantee the uniqueness and consistency of probability assignments obtained by different methods, statistical mechanics and logical inference in particular. Strictly speaking, the trial distributions, which do not maximize the entropy, are actually not probability distributions.
The maximum entropy principle makes explicit our freedom in using different forms of prior information. As a special case, a uniform prior probability density (Laplace's principle of indifference) may be adopted. Thus, the maximum entropy principle is not just an alternative to the methods of inference of classical statistics, but its important conceptual generalization and correction.

[edit] Testable information
The principle of maximum entropy is useful explicitly only when applied to testable information. A piece of information is testable if it can be determined whether a given distribution is consistent with it. For example, the statements

The expectation of the variable x is 2.87

and

p2 + p3 > 0.6

are statements of testable information.
Given testable information, the maximum entropy procedure consists of seeking the probability distribution which maximizes information entropy, subject to the constraints of the information. This constrained optimization problem is typically solved using the method of Lagrange multipliers.
Entropy maximization with no testable information takes place under a single constraint: the sum of the probabilities must be one. Under this constraint, the maximum entropy probability distribution is the uniform distribution,



The principle of maximum entropy can thus be seen as a generalization of the classical principle of indifference, also known as the principle of insufficient reason

[edit] General solution for the maximum entropy distribution with linear constraints

[edit] Discrete case
We have some testable information I about a quantity x taking values in {x1, x2,..., xn}. We express this information as m constraints on the expectations of the functions fk; that is, we require our epistemic probability distribution to satisfy



Furthermore, the probabilities must sum to one, giving the constraint



The probability distribution with maximum information entropy subject to these constraints is



It is sometimes called the Gibbs distribution. The normalization constant is determined by



and is conventionally called the partition function. (Interestingly, the Pitman-Koopman theorem states that the necessary and sufficient condition for a sampling distribution to admit sufficient statistics of bounded dimension is that it have the general form of a maximum entropy distribution.)
The λk parameters are Lagrange multipliers whose particular values are determined by the constraints according to



These m simultaneous equations do not generally possess a closed form solution, and are usually solved by numerical methods.

[edit] Continuous case
For continuous distributions, the simple definition of Shannon entropy ceases to be so useful (see differential entropy). Instead what is more useful is the relative entropy of the distribution, (Jaynes, 1963, 1968, 2003),



where m(x), which Jaynes called the "invariant measure", is proportional to the limiting density of discrete points. For now, we shall assume that it is known; we will discuss it further after the solution equations are given.
Relative entropy is usually defined as the Kullback-Leibler divergence of m from p (although it is sometimes, confusingly, defined as the negative of this). The inference principle of minimizing this, due to Kullback, is known as the Principle of Minimum Discrimination Information.
We have some testable information I about a quantity x which takes values in some interval of the real numbers (all integrals below are over this interval). We express this information as m constraints on the expectations of the functions fk, i.e. we require our epistemic probability density function to satisfy



And of course, the probability density must integrate to one, giving the constraint



The probability density function with maximum Hc subject to these constraints is



with the partition function determined by



As in the discrete case, the values of the λk parameters are determined by the constraints according to



The invariant measure function m(x) can be best understood by supposing that x is known to take values only in the bounded interval (a, b), and that no other information is given. Then the maximum entropy probability density function is



where A is a normalization constant. The invariant measure function is actually the prior density function encoding 'lack of relevant information'. It cannot be determined by the principle of maximum entropy, and must be determined by some other logical method, such as the principle of transformation groups or marginalization theory.
Refer to Cover and Thomas for excellent explanation of the ideas .

[edit] Examples
For several examples of maximum entropy distributions, see the article on maximum entropy probability distributions.

[edit] Justifications for the principle of maximum entropy
Proponents of the principle of maximum entropy justify its use in assigning epistemic probabilities in several ways, including the following two arguments. These arguments take the use of epistemic probability as given, and thus have no force if the concept is itself under question.

[edit] Information entropy as a measure of 'uninformativeness'
Consider a discrete epistemic probability distribution among m mutually exclusive propositions. The most informative distribution would occur when one of the propositions was known to be true. In that case, the information entropy would be equal to zero. The least informative distribution would occur when there is no reason to favor any one of the propositions over the others. In that case, the only reasonable probability distribution would be uniform, and then the information entropy would be equal to its maximum possible value, log m. The information entropy can therefore be seen as a numerical measure which describes how uninformative a particular probability distribution is from zero (completely informative) to log m (completely uninformative).
By choosing to use the distribution with the maximum entropy allowed by our information, the argument goes, we are choosing the most uninformative distribution possible. To choose a distribution with lower entropy would be to assume information we do not possess; to choose one with a higher entropy would violate the constraints of the information we do possess. Thus the maximum entropy distribution is the only reasonable distribution.

[edit] The Wallis derivation
The following argument is the result of a suggestion made by Graham Wallis to E. T. Jaynes in 1962 (Jaynes, 2003). It is essentially the same mathematical argument used for the Maxwell-Boltzmann statistics in statistical mechanics, although the conceptual emphasis is quite different. It has the advantage of being strictly combinatorial in nature, making no reference to information entropy as a measure of 'uncertainty', 'uninformativeness', or any other imprecisely defined concept. The information entropy function is not assumed a priori, but rather is found in the course of the argument; and the argument leads naturally to the procedure of maximizing the information entropy, rather than treating it in some other way.
Suppose an individual wishes to make an epistemic probability assignment among m mutually exclusive propositions. She has some testable information, but is not sure how to go about including this information in her probability assessment. She therefore conceives of the following random experiment. She will distribute N quanta of epistemic probability (each worth 1/N) at random among the m possibilities. (One might imagine that she will throw N balls into m buckets while blindfolded. In order to be as fair as possible, each throw is to be independent of any other, and every bucket is to be the same size.) Once the experiment is done, she will check if the probability assignment thus obtained is consistent with her information. If not, she will reject it and try again. Otherwise, her assessment will be



where ni is the number of quanta that were assigned to the ith proposition.
Now, in order to reduce the 'graininess' of the epistemic probability assignment, it will be necessary to use quite a large number of quanta of epistemic probability. Rather than actually carry out, and possibly have to repeat, the rather long random experiment, our protagonist decides to simply calculate and use the most probable result. The probability of any particular result is the multinomial distribution,



where



is sometimes known as the multiplicity of the outcome.
The most probable result is the one which maximizes the multiplicity W. Rather than maximizing W directly, our protagonist could equivalently maximize any monotonic increasing function of W. She decides to maximize



At this point, in order simplify the expression, our protagonist takes the limit as , i.e. as the epistemic probability levels go from grainy discrete values to smooth continuous values. Using Stirling's approximation, she finds



All that remains for our protagonist to do is to maximize entropy under the constraints of her testable information. She has found that the maximum entropy distribution is the most probable of all "fair" random epistemic distributions, in the limit as the probability levels go from discrete to continuous.

[edit] Compatibility with Bayes Rule
Giffin et al. (2007) state that Bayes' Rule and the Principle of Maximum Entropy (MaxEnt) are completely compatible and can be seen as special cases of the Method of Maximum (relative) Entropy. They state that this method reproduces every aspect of orthodox Bayesian inference methods. In addition this new method opens the door to tackling problems that could not be addressed by either the MaxEnt or orthodox Bayesian methods individually. Moreover, recent contributions (Lazar 2003, and Schennach 2005) show that frequentist relative-entropy-based inference approaches (such as Empirical Likelihood and Exponentially Tilted Empirical Likelihood - see e.g. Owen 2001 and Kitamura 2006) can be combined with prior information to perform Bayesian posterior analysis.
On the other hand, Jaynes (1986, 1996) showed the Bayesian methods can lead to results differing from the maximum-entropy ones. Thus the relationship between the two methods, in particular the question whether one can be derived from the other, still needs clarification.

[edit] See also

Entropy maximization
Maximum entropy classifier
Maximum entropy spectral estimation


[edit] References

Jaynes, E.T., 1957, 'Information Theory and Statistical Mechanics', in Physical Review May 1957 Volume 106, #4 (pages 620-630).
Jaynes, E. T., 1963, 'Information Theory and Statistical Mechanics', in Statistical Physics, K. Ford (ed.), Benjamin, New York, p. 181.
Jaynes, E. T., 1968, 'Prior Probabilities', IEEE Trans. on Systems Science and Cybernetics, SSC-4, 227.
Jaynes, E. T., 1986 (new version online 1996), 'Monkeys, kangaroos and N', in Maximum-Entropy and Bayesian Methods in Applied Statistics, J. H. Justice (ed.), Cambridge University Press, Cambridge, p. 26.
Jaynes, E. T., 2003, Probability Theory: The Logic of Science, Cambridge University Press.
Giffin, A. and Caticha, A., 2007, Updating Probabilities with Data and Moments
Guiasu, S. and Shenitzer, A., 1985, 'The principle of maximum entropy', The Mathematical Intelligencer, 7(1).
Harremoës P. and Topsøe F., 2001, Maximum Entropy Fundamentals, Entropy, 3(3), 191-226.
Kapur, J. N.; and Kesevan, H. K., 1992, Entropy optimization principles with applications, Boston: Academic Press. ISBN 0-12-397670-7
Kitamura, Y., 2006, Empirical Likelihood Methods in Econometrics: Theory and Practice,Cowles Foundation Discussion Papers 1569, Cowles Foundation, Yale University.
Lazar, N., 2003, "Bayesian Empirical Likelihood", Biometrika, 90, 319-326.
Owen, A. B., Empirical Likelihood, Chapman and Hall.
Schennach, S. M., 2005, "Bayesian Exponentially Tilted Empirical Likelihood", Biometrika, 92(1), 31-46.
Uffink, Jos, 1995, 'Can the Maximum Entropy Principle be explained as a consistency requirement?', Studies in History and Philosophy of Modern Physics 26B, 223-261.


[edit] External links

An easy-to-read introduction to maximum entropy methods in the context of natural language processing is:


Ratnaparkhi A. "A simple introduction to maximum entropy models for natural language processing" Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania, 1997. Available here.


Maximum Entropy Modeling

A maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro.[1]

This page contains pointers to various papers and software implementations of Maximum Entropy Model on the net.




Retrieved from "http://en.wikipedia.org/wiki/Principle_of_maximum_entropy"
Categories: Entropy and information | Machine learning | Statistical theory | Bayesian statistics | Statistical principlesHidden categories: Articles lacking in-text citations from September 2008 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Deutsch
Français
日本語









 This page was last modified on 21 April 2009, at 07:41 (UTC).
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
