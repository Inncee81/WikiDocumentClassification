













Metaheuristic - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Metaheuristic";
		var wgTitle = "Metaheuristic";
		var wgAction = "view";
		var wgArticleId = "774458";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 280149623;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Metaheuristic

From Wikipedia, the free encyclopedia

Jump to: navigation, search 
A metaheuristic is a heuristic method for solving a very general class of computational problems by combining user-given black-box procedures — usually heuristics themselves — in the hope of obtaining a more efficient or more robust procedure. The name combines the Greek prefix "meta" ("beyond", here in the sense of "higher level") and "heuristic" (from ευρισκειν, heuriskein, "to find").
Metaheuristics are generally applied to problems for which there is no satisfactory problem-specific algorithm or heuristic; or when it is not practical to implement such a method. Most commonly used metaheuristics are targeted to combinatorial optimization problems, but of course can handle any problem that can be recast in that form, such as solving boolean equations.




Contents


1 Overview
2 Timeline
3 Meta-heuristics concepts
4 General criticisms
5 Pragmatics
6 See also
7 References
8 Further reading
9 External links





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Overview
The goal of combinatorial optimization is to find a discrete mathematical object (such as a bit string or permutation) that maximizes (or minimizes) an arbitrary function specified by the user of the metaheuristic. These objects are generically called states, and the set of all candidate states is the search space. The nature of the states and the search space are usually problem-specific.
The function to be optimized is called the goal function, or objective function, and is usually provided by the user as a black-box procedure that evaluates the function on a given state. Depending on the meta-heuristic, the user may have to provide other black-box procedures that, say, produce a new random state, produce variants of a given state, pick one state among several, provide upper or lower bounds for the goal function over a set of states, and the like.
Some metaheuristics maintain at any instant a single current state, and replace that state by a new one. This basic step is sometimes called a state transition or move. The move is uphill or downhill depending on whether the goal function value increases or decreases. The new state may be constructed from scratch by a user-given generator procedure. Alternatively, the new state be derived from the current state by a user-given mutator procedure; in this case the new state is called a neighbour of the current one. Generators and mutators are often probabilistic procedures. The set of new states that can be produced by the mutator is the neighbourhood of the current state.
More sophisticated meta-heuristics maintain, instead of a single current state, a current pool with several candidate states. The basic step then may add or delete states from this pool. User-given procedures may be called to select the states to be discarded, and to generate the new ones to be added. The latter may be generated by combination or crossover of two or more states from the pool.
A metaheuristic also keep track of the current optimum, the optimum state among those already evaluated so far.
Since the set of candidates is usually very large, metaheuristics are typically implemented so that they can be interrupted after a client-specified time budget. If not interrupted, some exact metaheuristics will eventually check all candidates, and use heuristic methods only to choose the order of enumeration; therefore, they will always find the true optimum, if their time budget is large enough. Other metaheuristics give only a weaker probabilistic guarantee, namely that, as the time budget goes to infinity, the probability of checking every candidate tends to 1.

[edit] Timeline





Timeline of main metaheuristics.



1952: first works on stochastics optimization methods[1].
1954: Barricelli carry out the first simulations of the evolution process and use them on general optimization problems[2].
1965: Rechenberg conceives the first algorithm using evolution strategies[3].
1966: Fogel, Owens et Walsh propose evolutionary programming[4].
1970: Hastings conceives the Metropolis-Hastings algorithm, which can sample any probability density function[5].
1975: John Holland proposes the first genetic algorithms[6].
1980: Smith describes genetic programming[7].
1983: based on Hastings's work, Kirkpatrick, Gelatt and Vecchi conceive simulated annealing[8].
1985: independently, Černý proposes the same algorithm[9].
1986: first mention of the term "meta-heuristic" by Fred Glover, during the conception of tabu search[10] :


La recherche avec tabou peut être vue comme une "méta-heuristique", superposée à une autre heuristique.
("The search with taboo may be viewed as a 'meta-heuristic', overlying another heuristic.")


1986: Farmer, Packard and Perelson work on artificial immune systems[11].
1988: the first conference on genetic algorithms is organized at the University of Illinois at Urbana-Champaign.
1988: works on the collective behaviour of ants finds an application in artificial intelligence[12].
1988: Koza register his first patent on genetic programming[13].
1989: Goldberg publishes one of the best known books on genetic algorithms[14].
1989: Evolver, the first optimisation software using the genetic algorithm, is released by the Axcelis company.
1989: The term "memetic algorithm" is first used by Moscato[15].
1991: the ant colony algorithms are proposed by Marco Dorigo, in his Ph.D. thesis[16].
1993: The journal Evolutionary Computation begins to be published by the MIT.
1995: Feo and Resende propose the greedy randomized adaptive search procedure[17].
1995: Kennedy and Eberhart conceive particle swarm optimization[18][19].
1996: Mühlenbein and Paaß work on estimation of distribution algorithms[20].
1997: Storn and Price propose a differential evolution algorithm[21].
1997: Rubinstein works on the cross entropy method[22].
1999 : Boettcher propose the extremal optimization[23].
2000: First interactive genetic algorithms[24].
2001: Geem, Kim, and Longanathan propose harmony search[25].
2004: Nakrani and Tovey describe the honey bee algorithm[26].
2008: Yang describes the firefly algorithm[27].
2008: Karaboga and Basturk describe the Artificial Bee Colony Algorithm[28].


[edit] Meta-heuristics concepts
Some well-known meta heuristics are

Random optimization
Local search
Greedy algorithm and hill-climbing
Random-restart hill climbing
Best-first search
Genetic algorithms
Simulated annealing
Tabu search
Ant colony optimization
GRASP
Stochastic Diffusion Search
Generalized extremal optimization
Harmony search
Variable Neighbourhood Search
A*

Innumerable variants and hybrids of these techniques have been proposed, and many more applications of metaheuristics to specific problems have been reported. This is an active field of research, with a considerable literature, a large community of researchers and users, and a wide range of applications.

[edit] General criticisms
While there are many computer scientists who are enthusiastic advocates of metaheuristics, there are also many who are highly critical of the concept and have little regard for much of the research that is done on it.
Those critics point out, for one thing, that the general goal of the typical metaheuristic — the efficient optimization of an arbitrary black-box function—cannot be solved efficiently, since for any metaheuristic M one can easily build a function f that will force M to enumerate the whole search space (or worse). Indeed, the "no-free-lunch theorem" says that over the set of all mathematically possible problems, each optimization algorithm will do on average as well as any other. Thus, at best, a specific metaheuristic can be efficient only for restricted classes of goal functions (usually those that are partially "smooth" in some sense). However, when these restrictions are stated at all, they either exclude most applications of interest, or make the problem amenable to specific solution methods that are much more efficient than the meta-heuristic.
Moreover, all metaheuristics rely on auxiliary procedures (producers, mutators, etc.) that are given by the user as black-box functions. It turns out that the effectiveness of a metaheuristic on a particular problem depends almost exclusively on these auxiliary functions, and very little on the metaheuristic itself. Given any two distinct metaheuristics M and N, and almost any goal function f, it is usually possible to write a set of auxiliary procedures that will make M find the optimum much more efficient than N, by many orders of magnitude; or vice-versa. In fact, since the auxiliary procedures are usually unrestricted, one can submit the basic step of metaheuristic M as the generator or mutator for N. Because of this extreme generality, one cannot say that any metaheuristic is better than any other, not even for a specific class of problems. In particular, no meta-heuristic can be shown to be better for any specific problem than brute force search, or the following "banal metaheuristic":

Call the user-provided state generator.
Print the resulting state.
Stop.

Finally, all metaheuristic optimization techniques are extremely crude when evaluated by the standards of (continuous) nonlinear optimization. Within this area, it is well-known that to find the optimum of a smooth function on n variables one must essentially obtain its Hessian matrix, the n by n matrix of its second derivatives. If the function is given as a black-box procedure, then one must call it about n2/2 times, and solve an n by n system of linear equations, before one can make the first useful step towards the minimum. However, none of the common metaheuristics incorporate or accommodate this procedure. At best, they can be seen as computing some crude approximation to the local gradient of the goal function, and moving more or less "downhill". But gradient-descent can be extremely inefficient for non-linear optimization. For example, consider the problem of finding a pair of numbers x,y that minimizes the quadratic function Q(x,y) = 1000000(x + y - 1000)2 + (x - y - 10)2. Gradient-descent methods will generally take a very long time to reach the minimum from, say, (1000,0); whereas Hessian-based methods will reach it in one step. Unfortunately, "narrow valley" functions like this one are increasingly likely to occur as the dimension of the space increases.
Even though meta-heuristics are often used for discrete or non-differentiable functions, or black-box functions whose derivatives are not available, they cannot be expected to be of any value unless there is some correlation between goal function values at nearby candidate solutions—in other words, unless the goal function has a globally smooth continuous component more or less hidden by the jumps and bumps created by the discreteness constraints. Yet none of the popular meta-heuristics uses the know-how of continuous optimization when trying to exploit that continuous component. For example, if the problem is to find two integers that minimize the Q function above, known meta-heuristics (including genetic ones) will fail to notice the overall quadratic behavior of Q, and will essentially behave as a random local search—or worse. (Note that this remark refers to the global behavior of the goal function, not the local smoothness of a continuous goal function with many local minima. Such local smoothness is most effectively exploited by using continuous optimization methods inside the generator/mutator procedures, so that the meta-heuristic only sees a discrete search space consisting of the local minima.)

[edit] Pragmatics
Independently of whether those criticisms are valid or not, metaheuristics can be terribly wasteful if used indiscriminately (so would be classical heuristics). Since their performance is critically dependent on the user-provided generators and mutators, one should concentrate on improving these procedures, rather than twiddling the parameters of sophisticated metaheuristics. A trivial metaheuristic with a good mutator will usually run circles around a sophisticated one with a poor mutator (and a good problem-specific heuristic will often do much better than both). In this area, more than in any other, a few hours of reading, thinking and programming can easily save months of computer time. On the other hand, this generalization does not necessarily extend equally to all problem domains. The use of genetic algorithms, for example, has produced evolved design solutions that exceed the best human-produced solutions despite years of theory and research. Problem domains falling into this category are often problems of combinatorial optimization and include the design of sorting networks, and evolved antennas, among others.

[edit] See also

Search-based software engineering
Hyper-heuristic


[edit] References


^ Robbins, H. and Monro, S., A Stochastic Approximation Method, Annals of Mathematical Statistics, vol. 22, pp. 400-407, 1951
^ Barricelli, Nils Aall, Esempi numerici di processi di evoluzione, Methodos, pp. 45-68, 1954
^ Rechenberg, I., Cybernetic Solution Path of an Experimental Problem, Royal Aircraft Establishment Library Translation, 1965
^ Fogel, L., Owens, A.J., Walsh, M.J., Artificial Intelligence through Simulated Evolution, Wiley, 1966
^ W.K. Hastings. Monte Carlo Sampling Methods Using Markov Chains and Their Applications, Biometrika, volume 57, issue 1, pages 97-109, 1970
^ Holland, John H., Adaptation in Natural and Artificial Systems, University of Michigan Press, Ann Arbor, 1975
^ Smith, S.F., A Learning System Based on Genetic Adaptive Algorithms, PhD dissertation (University of Pittsburgh), 1980
^ S. Kirkpatrick, C. D. Gelatt et M. P. Vecchi, Optimization by Simulated Annealing, Science, volume 220, issue 4598, pages 671-680, 1983
^ V. Černý A thermodynamical approach to the travelling salesman problem : an efficient simulation algorithm Journal of Optimization Theory and Applications, volume45, pages 41-51, 1985
^ Fred GLover, Future Paths for Integer Programming and Links to Artificial Intelligence, Comput. & Ops. Res.Vol. 13, No.5, pp. 533-549, 1986
^ J.D. Farmer, N. Packard and A. Perelson, The immune system, adaptation and machine learning, Physica D, vol. 22, pp. 187--204, 1986
^ F. Moyson, B. Manderick, The collective behaviour of Ants : an Example of Self-Organization in Massive Parallelism, Actes de AAAI Spring Symposium on Parallel Models of Intelligence, Stanford, Californie, 1988
^ Koza, John R. Non-Linear Genetic Algorithms for Solving Problems. United States Patent 4,935,877. Filed May 20, 1988. Issued June 19, 1990
^ Goldberg, David E., Genetic Algorithms in Search, Optimization and Machine Learning, Kluwer Academic Publishers, Boston, MA., 1989
^ P. Moscato, On Evolution, Search, Optimization, Genetic Algorithms and Martial Arts : Towards Memetic Algorithms, Caltech Concurrent Computation Program, C3P Report 826, 1989.
^ M. Dorigo, Optimization, Learning and Natural Algorithms, Ph.D. Thesis, Politecnico di Milano, Italy, 1992.
^ Feo, T., Resende, M., Greedy randomized adaptive search procedure, Journal of Global Optimization, tome 42, page 32--37, 1992
^ Eberhart, R. C. et Kennedy, J., A new optimizer using particle swarm theory, Proceedings of the Sixth International Symposium on Micromachine and Human Science, Nagoya, Japan. pp. 39-43, 1995
^ Kennedy, J. et Eberhart, R. C., Particle swarm optimization, Proceedings of IEEE International Conference on Neural Networks, Piscataway, NJ. pp. 1942-1948, 1995
^ Mülhenbein, H., Paaß, G., From recombination of genes to the estimation of distribution I. Binary parameters, Lectures Notes in Computer Science 1411: Parallel Problem Solving from Nature, tome PPSN IV, pages 178--187, 1996
^ Rainer Storn, Kenneth Price, Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces, Journal of Global Optimization, volume 11, issue 4, pages 341-359, 1997
^ Rubinstein, R.Y., Optimization of Computer simulation Models with Rare Events, European Journal of Operations Research, 99, 89-112, 1997
^ Stefan Boettcher, Allon G. Percus, "Extremal Optimization : Methods derived from Co-Evolution", Proceedings of the Genetic and Evolutionary Computation Conference (1999)
^ Takagi, H., Active user intervention in an EC Search, Proceesings of the JCIS 2000
^ Geem Z. W., Kim J. H., and Loganathan G. V.,A new heuristic optimization algorithm: harmony search, Simulation, vol. 76, 60 (2001)
^ Nakrani S. and Tovey S., On honey bees and dynamic server allocation in Internet hosting centers, Adaptive Behaviour, vol. 12, 223 (2004)
^ Yang X. S., Firefly algorithm (chapter 8) in: Nature-inspired Metaheuristic Algorithms, Luniver Press, (2008)
^ Karaboga D. and Basturk B., On the performance of artificial bee colony algorithm, Applied Soft Computing, vol. 8, 687 (2008)



[edit] Further reading

C. Blum and A. Roli (2003). Metaheuristics in combinatorial optimization: Overview and conceptual comparison. ACM Computing Surveys 35(3) 268–308.
Geem Z. W., Kim J. H., and Loganathan G. V., A new heuristic optimization algorithm: harmony search, Simulation, vol. 76, 60 (2001)
Yang X. S., Firefly algorithm (chapter 8) in: Nature-inspired Metaheuristic Algorithms, Luniver Press, (2008).
Karaboga D. and Basturk B., On the performance of artificial bee colony algorithm, Applied Soft Computing, vol. 8, 687 (2008).


[edit] External links

ParadisEO: a C++ framework dedicated to the reusable design of metaheuristics, as well as hybrid, parallel and distributed metaheuristics.
EU/ME EU/ME (the EURO chapter on metaheuristics) is the largest working group on this topic. The website of EU/ME is the main platform for communication among metaheuristics researchers.
DGPF A distributed framework for randomized, heuristic searches like GA and Hill Climbing which comes with a specialization for Genetic Programming and allows to combine different search algorithms.
MHTB A toolbox of metaheuristic algorithms for MATLAB. It offers single-solution, population-based and hybrids metaheuristics. With this toolbox you can solve optimization problems defined in the MATLAB language using metaheuristic algorithms implemented in C++ and Java.
jMetal jMetal is an object-oriented Java-based framework aimed at the development, experimentation, and study of metaheuristics for solving multi-objective optimization problems.
Metaheuristic / Stochastic Local Search Forum A forum where practitioners and researchers can discuss and share knowledge about metaheuristics and stochastic local search algorithms.




Retrieved from "http://en.wikipedia.org/wiki/Metaheuristic"
Categories: Applied mathematics | Operations research | Mathematical optimization | Heuristics 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Deutsch
Español
Français
日本語
Português
Русский









 This page was last modified on 28 March 2009, at 04:18.
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
