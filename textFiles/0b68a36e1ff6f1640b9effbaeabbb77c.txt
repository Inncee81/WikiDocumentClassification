













Machine translation - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Machine_translation";
		var wgTitle = "Machine translation";
		var wgAction = "view";
		var wgArticleId = "19980";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 281219073;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Machine translation

From Wikipedia, the free encyclopedia

Jump to: navigation, search 





This article needs additional citations for verification. Please help improve this article by adding reliable references (ideally, using inline citations). Unsourced material may be challenged and removed. (June 2008)


Machine translation, sometimes referred to by the abbreviation MT, is a sub-field of computational linguistics that investigates the use of computer software to translate text or speech from one natural language to another. At its basic level, MT performs simple substitution of words in one natural language for words in another. Using corpus techniques, more complex translations may be attempted, allowing for better handling of differences in linguistic typology, phrase recognition, and translation of idioms, as well as the isolation of anomalies.
Current machine translation software often allows for customisation by domain or profession (such as weather reports) — improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows then that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.
Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators, and in some cases can even produce output that can be used "as is".




Contents


1 History
2 Translation process
3 Approaches

3.1 Rule-based
3.2 Statistical
3.3 Example-based


4 Major issues

4.1 Disambiguation
4.2 Named entities


5 Applications
6 Evaluation
7 See also
8 Notes
9 References
10 External links

10.1 Software







//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] History
Main article: History of machine translation
The idea of machine translation may be traced back to 17th century. In 1629, René Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol. In the 1950s, The Georgetown experiment (1954) involved fully-automatic translation of over sixty Russian sentences into English. The experiment was a great success and ushered in an era of substantial funding for machine-translation research. The authors claimed that within three to five years, machine translation would be a solved problem.
Real progress was much slower, however, and after the ALPAC report (1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced. Beginning in the late 1980s, as computational power increased and became less expensive, more interest was shown in statistical models for machine translation.
The idea of using digital computers for translation of natural languages was proposed as early as 1946 by A. D. Booth and possibly others. The Georgetown experiment was by no means the first such application, and a demonstration was made in 1954 on the APEXC machine at Birkbeck College (University of London) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (see for example Wireless World, Sept. 1955, Cleave and Zacharov). A similar application, also pioneered at Birkbeck College at the time, was reading and composing Braille texts by computer.

[edit] Translation process
Main article: Translation process
The translation process may be stated as:

Decoding the meaning of the source text; and
Re-encoding this meaning in the target language.

Behind this ostensibly simple procedure lies a complex cognitive operation. To decode the meaning of the source text in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the grammar, semantics, syntax, idioms, etc., of the source language, as well as the culture of its speakers. The translator needs the same in-depth knowledge to re-encode the meaning in the target language.
Therein lies the challenge in machine translation: how to program a computer that will "understand" a text as a person does, and that will "create" a new text in the target language that "sounds" as if it has been written by a person.
This problem may be approached in a number of ways.

[edit] Approaches




Pyramid showing comparative depths of intermediary representation, interlingual machine translation at the peak, followed by transfer-based, then direct translation.


Machine translation can use a method based on linguistic rules, which means that words will be translated in a linguistic way — the most suitable (orally speaking) words of the target language will replace the ones in the source language.
It is often argued that the success of machine translation requires the problem of natural language understanding to be solved first.
Generally, rule-based methods parse a text, usually creating an intermediary, symbolic representation, from which the text in the target language is generated. According to the nature of the intermediary representation, an approach is described as interlingual machine translation or transfer-based machine translation. These methods require extensive lexicons with morphological, syntactic, and semantic information, and large sets of rules.
Given enough data, machine translation programs often work well enough for a native speaker of one language to get the approximate meaning of what is written by the other native speaker. The difficulty is getting enough data of the right kind to support the particular method. For example, the large multilingual corpus of data needed for statistical methods to work is not necessary for the grammar-based methods. But then, the grammar methods need a skilled linguist to carefully design the grammar that they use.
To translate between closely related languages, a technique referred to as shallow-transfer machine translation may be used.

[edit] Rule-based
The rule-based machine translation paradigm includes transfer-based machine translation, interlingual machine translation and dictionary-based machine translation paradigms.
Main article: Rule-based machine translation
Transfer-based machine translation
Main article: Transfer-based machine translation
Interlingual
Main article: Interlingual machine translation
Interlingual machine translation is one instance of rule-based machine-translation approaches. In this approach, the source language, i.e. the text to be translated, is transformed into an interlingual, i.e. source-/target-language-independent representation. The target language is then generated out of the interlingua.
Dictionary-based
Main article: Dictionary-based machine translation
Machine translation can use a method based on dictionary entries, which means that the words will be translated as they are by a dictionary.

[edit] Statistical
Main article: Statistical machine translation
Statistical machine translation tries to generate translations using statistical methods based on bilingual text corpora, such as the Canadian Hansard corpus, the English-French record of the Canadian parliament and EUROPARL, the record of the European Parliament. Where such corpora are available, impressive results can be achieved translating texts of a similar kind, but such corpora are still very rare. The first statistical machine translation software was CANDIDE from IBM. Google used SYSTRAN for several years, but has switched to a statistical translation method in October 2007. Recently, they improved their translation capabilities by inputting approximately 200 billion words from United Nations materials to train their system. Accuracy of the translation has improved. [1]

[edit] Example-based
Main article: Example-based machine translation
Example-based machine translation (EBMT) approach is often characterised by its use of a bilingual corpus as its main knowledge base, at run-time. It is essentially a translation by analogy and can be viewed as an implementation of case-based reasoning approach of machine learning.

[edit] Major issues

[edit] Disambiguation
Main article: Word sense disambiguation
Word-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning. The problem was first raised in the 1950s by Yehoshua Bar-Hillel.[2] He pointed out that without a "universal encyclopedia", a machine would never be able to distinguish between the two meanings of a word.[3] Today there are numerous approaches designed to overcome this problem. They can be approximately divided into "shallow" approaches and "deep" approaches.
Shallow approaches assume no knowledge of the text. They simply apply statistical methods to the words surrounding the ambiguous word. Deep approaches presume a comprehensive knowledge of the word. So far, shallow approaches have been more successful.[citation needed]
The late Claude Piron, a long-time translator for the United Nations and the World Health Organization, wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved:

Why does a translator need a whole workday to translate five pages, and not an hour or two? ..... About 90% of an average text corresponds to these simple conditions. But unfortunately, there's the other 10%. It's that part that requires six [more] hours of work. There are the ambiguities one has to resolve. For instance, the author of the source text, an Australian physician, cited the example of an epidemic which was declared during World War II in a "Japanese prisoner of war camp". Was he talking about an American camp with Japanese prisoners or a Japanese camp with American prisoners? The English has two senses. It's necessary therefore to do research, maybe to the extent of a phone call to Australia. [4]

The ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree of AI than has yet been attained. A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often. A shallow approach that involves "ask the user about each ambiguity" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human.

[edit] Named entities
Related to named entity recognition in information extraction.

[edit] Applications
There are now many software programs for translating natural language, several of them online, such as:

SYSTRAN, which powers Yahoo's Babel Fish
Promt, which powers online translation services at Voila.fr and Orange.fr

Although no system provides the holy grail of fully automatic high-quality machine translation, many systems produce reasonable output.
Despite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is the European Commission.
Toggletext uses a transfer-based system (known as Kataku) to translate between English and Indonesian.
Google has claimed that promising results were obtained using a proprietary statistical machine translation engine.[5] The statistical translation engine used in the Google language tools for Arabic <-> English and Chinese <-> English has an overall score of 0.4281 over the runner-up IBM's BLEU-4 score of 0.3954 (Summer 2006) in tests conducted by the National Institute for Standards and Technology.[6][7][8]
With the recent focus on terrorism, the military sources in the United States have been investing significant amounts of money in natural language engineering. In-Q-Tel[9] (a venture capital fund, largely funded by the US Intelligence Community, to stimulate new technologies through private sector entrepreneurs) brought up companies like Language Weaver. Currently the military community is interested in translation and processing of languages like Arabic, Pashto, and Dari.[citation needed] Information Processing Technology Office in DARPA hosts programs like TIDES and Babylon Translator. US Air Force has awarded a $1 million contract to develop a language translation technology.[10]

[edit] Evaluation
Main article: Evaluation of machine translation
There are various means for evaluating the performance of machine-translation systems. The oldest is the use of human judges[11] to assess a translation's quality. Even though human evaluation is time-consuming, it is still the most reliable way to compare different systems such as rule-based and statistical systems. Automated means of evaluation include BLEU, NIST and METEOR.
Relying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded, and that it takes a human to adequately comprehend the context of the original text. Even purely human-generated translations are prone to error. Therefore, to ensure that a machine-generated translation will be of publishable quality and useful to a human, it must be reviewed and edited by a human.
It has, however, been asserted that in certain applications, e.g. product descriptions written in a controlled language, a dictionary-based machine-translation system has produced satisfactory translations that require no human intervention.[12]

[edit] See also

Comparison of Machine translation applications
Artificial Intelligence
Computational linguistics
Universal Networking Language
Computer-assisted translation and Translation memory
Controlled natural language
History of machine translation
Human Language Technology
List of emerging technologies
List of research laboratories for machine translation
Pseudo-translation
Translation
Universal translator
Wiktionary:Translations
Phraselator
Mobile translation


[edit] Notes


^ Google Translator: The Universal Language
^ Milestones in machine translation - No.6: Bar-Hillel and the nonfeasibility of FAHQT by John Hutchins
^ Bar-Hillel (1960), "Automatic Translation of Languages". Available online at http://www.mt-archive.info/Bar-Hillel-1960.pdf
^ Claude Piron, Le défi des langues (The Language Challenge), Paris, L'Harmattan, 1994.
^ Google Blog: The machines do the translating (by Franz Och)
^ Geer, David, "Statistical Translation Gains Respect", pp. 18 - 21, IEEE Computer, October 2005
^ Ratcliff, Evan "Me Translate Pretty One Day", Wired December 2006
^ "NIST 2006 Machine Translation Evaluation Official Results", November 1, 2006
^ In-Q-Tel
^ GCN — Air force wants to build a universal translator
^ Comparison of MT systems by human evaluation, May 2008
^ Muegge (2006), "Fully Automatic High Quality Machine Translation of Restricted Text: A Case Study," in Translating and the computer 28. Proceedings of the twenty-eighth international conference on translating and the computer, 16-17 November 2006, London, London: Aslib. ISBN 978-0-85142-483-5.



[edit] References

Hutchins, W. John; and Harold L. Somers (1992). An Introduction to Machine Translation. London: Academic Press. ISBN 0-12-362830-X. http://www.hutchinsweb.me.uk/IntroMT-TOC.htm. 
Claude Piron, Le défi des langues — Du gâchis au bon sens (The Language Challenge: From Chaos to Common Sense), Paris, L'Harmattan, 1994.


[edit] External links



Wikiversity has learning materials about Topic:Computational linguistics



International Association for Machine Translation (IAMT)
Machine Translation, an introductory guide to MT by D.J.Arnold et al. (1994)
Machine Translation Archive by John Hutchins. An electronic repository (and bibliography) of articles, books and papers in the field of machine translation and computer-based translation technology
Machine translation (computer-based translation) — Publications by John Hutchins (includes PDFs of several books on machine translation)
Machine Translation and Minority Languages
John Hutchins 1999


[edit] Software

Apertium - an open-source shallow-transfer machine translation engine and toolbox
A Punjabi To Hindi Machine Translation System
Moses - an open source toolkit for phrase-based statistical machine translation



Approaches to Machine translation


Dictionary-based · Rule-based (RBMT) · Transfer-based · Statistical (SMT) · Example-based (EBMT) · Interlingual







Retrieved from "http://en.wikipedia.org/wiki/Machine_translation"
Categories: Artificial intelligence applications | Computational linguistics | Machine translation | Computer-assisted translation | Natural language processingHidden categories: Articles needing additional references from June 2008 | All articles with unsourced statements | Articles with unsourced statements since April 2007 | Articles with unsourced statements since February 2007 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Afrikaans
العربية
Беларуская
Беларуская (тарашкевіца)
Български
Català
Česky
Cymraeg
Dansk
Deutsch
Español
Esperanto
Euskara
فارسی
Français
한국어
हिन्दी
Hrvatski
Bahasa Indonesia
Italiano
עברית
Lietuvių
Magyar
Bahasa Melayu
Nederlands
日本語
‪Norsk (bokmål)‬
Occitan
Polski
Português
Română
Русский
Simple English
Slovenčina
Српски / Srpski
Suomi
Svenska
தமிழ்
ไทย
Tiếng Việt
Тоҷикӣ
Українська
吴语
粵語
中文









 This page was last modified on 2 April 2009, at 03:24.
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
