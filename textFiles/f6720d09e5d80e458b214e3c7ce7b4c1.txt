













Fisher information - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Fisher_information";
		var wgTitle = "Fisher information";
		var wgAction = "view";
		var wgArticleId = "598971";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 273602656;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Fisher information

From Wikipedia, the free encyclopedia

Jump to: navigation, search 
In statistics and information theory, the Fisher information (denoted ) is the variance of the score. It is named in honor of its inventor, the statistician R.A. Fisher.




Contents


1 Definition

1.1 Informal derivation
1.2 Single-parameter Bernoulli experiment


2 Matrix form

2.1 Orthogonal parameters
2.2 Multivariate normal distribution


3 Properties
4 See also
5 Notes
6 References
7 Further weblinks





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Definition
The Fisher information is a way of measuring the amount of information that an observable random variable X carries about an unknown parameter θ upon which the likelihood function of θ, L(θ) = f(X;θ), depends. The likelihood function is the joint probability of the data, the Xs, conditional on the value of θ, as a function of θ. Since the expectation of the score is zero, the variance is simply the second moment of the score, the derivative of the log of the likelihood function with respect to θ. Hence the Fisher information can be written



which implies . The Fisher information is thus the expectation of the squared score. A random variable carrying high Fisher information implies that the absolute value of the score is often high.
The Fisher information is not a function of a particular observation, as the random variable X has been averaged out. The concept of information is useful when comparing two methods of observing a given random process.
If lnf(x;θ) is twice differentiable with respect to θ, and if the regularity condition



holds, then the Fisher information may also be written as[1]



Thus Fisher information is the negative of the expectation of the second derivative of the log of f with respect to θ. Information may thus be seen to be a measure of the "sharpness" of the support curve near the maximum likelihood estimate of θ. A "blunt" support curve (one with a shallow maximum) would have a low expected second derivative, and thus low information; while a sharp one would have a high expected second derivative and thus high information.
Information is additive, in that the information yielded by two independent experiments is the sum of the information from each experiment separately:



This result follows from the elementary fact that if random variables are independent, the variance of their sum is the sum of their variances. Hence the information in a random sample of size n is n times that in a sample of size 1 (if observations are independent).
The information provided by a sufficient statistic is the same as that of the sample X. This may be seen by using Neyman's factorization criterion for a sufficient statistic. If T(X) is sufficient for θ, then



for some functions g and h. See sufficient statistic for a more detailed explanation. The equality of information then follows from the following fact:



which follows from the definition of Fisher information, and the independence of h(X) from θ. More generally, if T = t(X) is a statistic, then



with equality if and only if T is a sufficient statistic.
The Cramér-Rao inequality states that the inverse of the Fisher information is a lower bound on the variance of any unbiased estimator of θ.

[edit] Informal derivation
Van Trees (1968) and Frieden (2004) provide the following method of deriving the Fisher information informally:
Consider an unbiased estimator . Mathematically, we write



The likelihood function f(X;θ) describes the probability that we observe a given sample x given a known value of θ. If f is sharply peaked, it is easy to intuit the "correct" value of θ given the data, and hence the data contains a lot of information about the parameter. If the likelihood f is flat and spread-out, then it would take many, many samples of X to estimate the actual "true" value of θ. Therefore, we would intuit that the data contain much less information about the parameter.
Now, we differentiate the unbiased-ness condition above to get



We now make use of two facts. The first is that the likelihood f is just the probability of the data given the parameter. Since it is a probability, it must be normalized, implying that



Second, we know from basic calculus that

.

Using these two facts in the above let us write



Factoring the integrand gives



If we square the equation, the Cauchy-Schwarz inequality lets us write



The right-most factor is defined to be the Fisher Information



The left-most factor is the expected mean-squared error of the estimator θ, since



Notice that the inequality tells us that, fundamentally,



In other words, the precision to which we can estimate θ is fundamentally limited by the Fisher Information of likelihood function.

[edit] Single-parameter Bernoulli experiment
A Bernoulli trial is a random variable with two possible outcomes, "success" and "failure", with "success" having a probability of θ. The outcome can be thought of as determined by a coin toss, with the probability of obtaining a "head" being θ and the probability of obtaining a "tail" being 1 − θ.
The Fisher information contained in n independent Bernoulli trials may be calculated as follows. In the following, A represents the number of successes, B the number of failures, and n = A + B is the total number of trials.




















 (on differentiating ln x, see logarithm) 













 (as the expected value of A = nθ, etc.) 










(1) defines Fisher information. (2) invokes the fact that the information in a sufficient statistic is the same as that of the sample itself. (3) expands the log term and drops a constant. (4) and (5) differentiate with respect to θ. (6) replaces A and B with their expectations. (7) is algebra.
The end result, namely,



is the reciprocal of the variance of the mean number of successes in n Bernoulli trials, as expected (see last sentence of the preceding section).

[edit] Matrix form
When there are N parameters, so that θ is a Nx1 vector  then the Fisher information takes the form of an NxN matrix, the Fisher Information Matrix (FIM), with typical element:



The FIM is a NxN positive definite symmetric matrix, defining a metric on the N-dimensional parameter space. Exploring this topic requires differential geometry.
If the following regularity condition is met:



then the Fisher Information Matrix may also be written as:




[edit] Orthogonal parameters
We say that two parameters θi and θj are orthogonal if the element of the i-th row and j-th column of the Fisher Information Matrix is zero. Orthogonal parameters are easy to deal with in the sense that their maximum likelihood estimates are independent and can be calculated separately. When dealing with research problems, it is very common for the researcher to invest some time searching for an orthogonal parametrization of the densities involved in the problem.

[edit] Multivariate normal distribution
The FIM for a N-variate multivariate normal distribution has a special form. Let  and let Σ(θ) be the covariance matrix. Then the typical element , 0 ≤ m, n < N, of the FIM for  is:



where  denotes the transpose of a vector, tr(..) denotes the trace of a square matrix, and:






Note that a special, but very common case is the one where Σ(θ) = Σ, a constant. Then
 .

In this case the Fisher information matrix may be identified with the coefficient matrix of the normal equations of least squares estimation theory.

[edit] Properties
The Fisher information depends on the parametrization of the problem. If θ and η are two different parameterizations of a problem, such that θ = h(η) and h is a differentiable function, then



where  and  are the Fisher information measures of η and θ, respectively.[2]

[edit] See also

Formation matrix
Jeffreys prior

Other measures employed in information theory:

Self-information
Kullback-Leibler divergence
Shannon entropy


[edit] Notes


^ Lehmann and Casella, eq. (2.5.16).
^ Lehmann and Casella, eq. (2.5.11).



[edit] References

Schervish, Mark J. (1995). Theory of Statistics. New York: Springer. Section 2.3.1. ISBN 0387945466. 


Van Trees, H. L. (1968). Detection, Estimation, and Modulation Theory, Part I. New York: Wiley. ISBN 0471095176. 


Lehmann, E. L.; Casella, G. (1998). Theory of Point Estimation. Springer. pp. 2nd ed. ISBN 0-387-98502-6. 


[edit] Further weblinks

Fisher4Cast: a Matlab, GUI-based Fisher information tool for research and teaching, primarily aimed at cosmological forecasting applications.




Retrieved from "http://en.wikipedia.org/wiki/Fisher_information"
Categories: Estimation theory | Information theory 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Deutsch
Français
Italiano
日本語
Русский









 This page was last modified on 27 February 2009, at 06:42.
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
