













Cache algorithms - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Cache_algorithms";
		var wgTitle = "Cache algorithms";
		var wgAction = "view";
		var wgArticleId = "954281";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 272107516;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Cache algorithms

From Wikipedia, the free encyclopedia

Jump to: navigation, search 
This article is about general cache algorithms.  For detailed algorithms specific to paging, see page replacement algorithm.

For detailed algorithms specific to the cache between a CPU and RAM, see CPU cache.

In computing, cache algorithms (also frequently called replacement algorithms or replacement policies) are optimizing instructions – algorithms – that a computer program or a hardware-maintained structure can follow to manage a cache of information stored on the computer. When the cache is full, the algorithm must choose which items to discard to make room for the new ones.
The "hit rate" of a cache describes how often a searched-for item is actually found in the cache. More efficient replacement policies keep track of more usage information in order to improve the hit rate (for a given cache size).
The "latency" of a cache describes how long after requesting a desired item the cache can return that item (when there is a hit). Faster replacement strategies typically keep track of less usage information -- or, in the case of direct-mapped cache, no information -- to reduce the amount of time required to update that information.
Each replacement strategy is a compromise between hit rate and latency.




Contents


1 Examples

1.1 Belady's Algorithm
1.2 Least Recently Used
1.3 Most Recently Used
1.4 Pseudo-LRU
1.5 Segmented LRU
1.6 2-Way Set Associative
1.7 Direct-mapped cache
1.8 Least-Frequently Used
1.9 Adaptive Replacement Cache
1.10 Multi Queue Caching Algorithm


2 See also
3 References
4 External links





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Examples

[edit] Belady's Algorithm
The most efficient caching algorithm would be to always discard the information that will not be needed for the longest time in the future. This optimal result is referred to as Belady's optimal algorithm or the clairvoyant algorithm. Since it is generally impossible to predict how far in the future information will be needed, this is generally not implementable in practice. The practical minimum can be calculated only after experimentation, and one can compare the effectiveness of the actually chosen cache algorithm with the optimal minimum.

[edit] Least Recently Used
Least Recently Used (LRU): discards the least recently used items first. This algorithm requires keeping track of what was used when, which is expensive if one wants to make sure the algorithm always discards the least recently used item. General implementations of this technique require to keep "age bits" for cache-lines and track the "Least Recently Used" cache-line based on age-bits. In such implementation, every time a cache-line is used, the age of all other cache-lines changes. LRU is actually a family of caching algorithms with members including: 2Q by Theodore Johnson and Dennis Shasha and LRU/K by Pat O'Neil, Betty O'Neil and Gerhard Weikum.

[edit] Most Recently Used
Most Recently Used (MRU): discards, in contrast to LRU, the most recently used items first. According to [1] "When a file is being repeatedly scanned in a [Looping Sequential] reference pattern, MRU is the best replacement algorithm" In [2] the authors also point out that for random access patterns and repeated scans over large datasets (sometimes known as cyclic access patterns) MRU cache algorithms have more hits than LRU due to their tendency to retain older data. MRU algorithms are most useful in situations where the older an item is the more likely it is to be accessed.

[edit] Pseudo-LRU
Pseudo-LRU (PLRU): For caches with large associativity (generally >4 ways), the implementation cost of LRU becomes prohibitive. If a probabilistic scheme that almost always discards one of the least recently used items is sufficient, the PLRU algorithm can be used which only needs one bit per cache item to work.




Which memory locations can be cached by which cache locations



[edit] Segmented LRU
Segmented LRU (SLRU): "An SLRU cache is divided into two segments. a probationary segment and a protected segment. Lines in each segment are ordered from the most to the least recently accessed. Data from misses is added to the cache at the most recently accessed end of the probationary segment. Hits are removed from wherever they currently reside and added to the most recently accessed end of the protected segment. Lines in the protected segment have thus been accessed at least twice. The protected segment is finite. so migration of a line from the probationary segment to the protected segment may force the migration of the LRU line in the protected segment to the most recently used (MRU) end of the probationary segment. giving this line another chance to be accessed before being replaced. The size limit on the protected segment is an SLRU parameter that varies according to the I/O workloadpatterns. Whenever data must be discarded from the cache, lines are obtained from the LRU end of the probationary segment.[3]"

[edit] 2-Way Set Associative
2-way set associative: for high-speed CPU caches where even PLRU is too slow. The address of a new item is used to calculate one of two possible locations in the cache where it is allowed to go. The LRU of the two is discarded. This requires one bit per pair of cache lines[citation needed], to indicate which of the two was the least recently used.

[edit] Direct-mapped cache
Direct-mapped cache: for the highest-speed CPU caches where even 2-way set associative caches are too slow. The address of the new item is used to calculate the one location in the cache where it is allowed to go. Whatever was there before is discarded.

[edit] Least-Frequently Used
Least Frequently Used (LFU): LFU counts how often an item is needed. Those that are used least often are discarded first.

[edit] Adaptive Replacement Cache
Adaptive Replacement Cache (ARC):[4] constantly balances between LRU and LFU, to improve combined result.

[edit] Multi Queue Caching Algorithm
Multi Queue (MQ) caching algorithm: [5] (by Y. Zhou J.F. Philbin and Kai Li).
Other things to consider:

Items with different cost: keep items that are expensive to obtain, e.g. those that take a long time to get.
Items taking up more cache: If items have different sizes, the cache may want to discard a large item to store several smaller ones.
Items that expire with time: Some caches keep information that expires (e.g. a news cache, a DNS cache, or a web browser cache). The computer may discard items because they are expired. Depending on the size of the cache no further caching algorithm to discard items may be necessary.

Various algorithms also exist to maintain cache coherency. This applies only to situation where multiple independent caches are used for the same data (for example multiple database servers updating the single shared data file).

[edit] See also

Cache
Cache-oblivious algorithm
CPU cache
Page replacement algorithm
Locality of reference


[edit] References

^ Hong-Tai Chou”. David J. Dewitt: http://www.vldb.org/conf/1985/P127.PDF
^ Semantic Data Caching and Replacement: http://www.vldb.org/conf/1996/P330.PDF
^ Ramakrishna Karedla, J. Spencer Love, and Bradley G. Wherry. Caching Strategies to Improve Disk System Performance. In Computer, 1994.
^ http://www.usenix.org/events/fast03/tech/full_papers/megiddo/megiddo.pdf
^ [1]


[edit] External links

Definitions of various cache algorithms
Fully associative cache
Set associative cache
Direct mapped cache




Retrieved from "http://en.wikipedia.org/wiki/Cache_algorithms"
Categories: Cache | Memory management algorithmsHidden categories: All articles with unsourced statements | Articles with unsourced statements since March 2008 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Deutsch
Bahasa Indonesia
日本語
Türkçe
中文
Français









 This page was last modified on 20 February 2009, at 18:28 (UTC).
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
