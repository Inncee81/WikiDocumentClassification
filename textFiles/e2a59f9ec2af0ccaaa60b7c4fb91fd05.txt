













Larrabee (GPU) - Wikipedia, the free encyclopedia














/*<![CDATA[*/
		var skin = "monobook";
		var stylepath = "/skins-1.5";
		var wgArticlePath = "/wiki/$1";
		var wgScriptPath = "/w";
		var wgScript = "/w/index.php";
		var wgVariantArticlePath = false;
		var wgActionPaths = {};
		var wgServer = "http://en.wikipedia.org";
		var wgCanonicalNamespace = "";
		var wgCanonicalSpecialPageName = false;
		var wgNamespaceNumber = 0;
		var wgPageName = "Larrabee_(GPU)";
		var wgTitle = "Larrabee (GPU)";
		var wgAction = "view";
		var wgArticleId = "11546286";
		var wgIsArticle = true;
		var wgUserName = null;
		var wgUserGroups = null;
		var wgUserLanguage = "en";
		var wgContentLanguage = "en";
		var wgBreakFrames = false;
		var wgCurRevisionId = 282762702;
		var wgVersion = "1.15alpha";
		var wgEnableAPI = true;
		var wgEnableWriteAPI = true;
		var wgSeparatorTransformTable = ["", ""];
		var wgDigitTransformTable = ["", ""];
		var wgMWSuggestTemplate = "http://en.wikipedia.org/w/api.php?action=opensearch\x26search={searchTerms}\x26namespace={namespaces}\x26suggest";
		var wgDBname = "enwiki";
		var wgSearchNamespaces = [0];
		var wgMWSuggestMessages = ["with suggestions", "no suggestions"];
		var wgRestrictionEdit = [];
		var wgRestrictionMove = [];
		/*]]>*/
<!-- wikibits js -->



/*<![CDATA[*/
var wgNotice='';var wgNoticeLocal='';
/*]]>*/ 
<!-- site js -->






if (wgNotice != '') document.writeln(wgNotice); Larrabee (GPU)

From Wikipedia, the free encyclopedia

Jump to: navigation, search 





This article contains information about scheduled or expected future computer chips.
It may contain preliminary or speculative information, and may not reflect the final specification of the product.









The Larrabee GPU architecture, unveiled at the SIGGRAPH conference in August 2008.


Larrabee is the codename for a graphics processing unit (GPU) chip that Intel is developing separately from its current line of integrated graphics accelerators. Larrabee is expected to compete with GeForce and Radeon products from NVIDIA and ATI respectively. Larrabee will also compete in the GPGPU and high-performance computing markets. The first video cards featuring Larrabee are likely to be released in early 2010.[1][2]




Contents


1 Comparison with competing products

1.1 Differences with current GPUs
1.2 Differences with CPUs
1.3 Comparison with the Cell Broadband Engine
1.4 Comparison with Intel GMA


2 Preliminary performance data
3 See also
4 References
5 External links





//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>


[edit] Comparison with competing products




Larrabee has a fully programmable pipeline, in contrast to current generation graphics cards which are only partially programmable.


Larrabee can be considered a hybrid between a multi-core CPU and a GPU, and has similarities to both. Its coherent cache hierarchy and x86 architecture compatibility are CPU-like, while its wide SIMD vector units and texture sampling hardware are GPU-like.
As a GPU, Larrabee will support traditional rasterized 3D graphics (DirectX/OpenGL) for games. However, Larrabee's hybrid of CPU and GPU features should be suitable for general purpose GPU (GPGPU) or stream processing tasks.[1] For example, Larrabee might perform ray tracing or physics processing,[3] in real time for games or offline for scientific research as a component of a supercomputer.[4]
DreamWorks Animation has partnered with Intel and is planning to use Larrabee in movie production. DreamWorks Animation CEO Jeffrey Katzenberg states "we are well on the way of upgrading our software to really take advantage of Larrabee and in terms of speed, flexibility, capacity, it just raises the bar of what we can do by not 2 or 3x, but 20x."[5]
Larrabee's early presentation has drawn some criticism from GPU competitors. At NVISION 08, several NVIDIA employees called Intel's SIGGRAPH paper about Larrabee "marketing puff" and told the press that the Larrabee architecture was "like a GPU from 2006".[6]

[edit] Differences with current GPUs
Larrabee will differ from other discrete GPUs currently on the market such as the GeForce 200 Series and the Radeon 4000 series in three major ways:

Larrabee will use the x86 instruction set with Larrabee-specific extensions.[7]


Larrabee will feature cache coherency across all its cores.[7]


Larrabee will include very little specialized graphics hardware, instead performing tasks like z-buffering, clipping, and blending in software, using a tile-based rendering approach.[7] A renderer implemented in software can more easily be modified, allowing more differentiation in appearance between games or other 3D applications. Intel's SIGGRAPH 2008 paper[7] mentions order-independent transparency, irregular Z-buffering, and real-time raytracing as rendering features that can be implemented with Larrabee.


[edit] Differences with CPUs
The x86 processor cores in Larrabee will be different in several ways from the cores in current Intel CPUs such as the Core 2 Duo or Core i7:

Larrabee's x86 cores will be based on the much simpler Pentium P54C design which is still being maintained for use in embedded applications. [8] The P54C-derived core is superscalar but does not include out-of-order execution, though it has been updated with modern features such as x86-64 support, [7] similarly to Intel Atom. In-order execution means lower performance for individual cores, but since they are smaller, more can fit on a single chip, increasing overall throughput (and lowering observed memory latency). Execution is also more deterministic so instruction and task scheduling can be done by compiler.


Each Larrabee core contains a 512-bit vector processing unit, able to process 16 single precision floating point numbers at a time. This is similar to but four times larger than the SSE units on most x86 processors, with additional features like scatter/gather instructions and a mask register designed to make using the vector unit easier and more efficient. Larrabee derives most of its number-crunching power from these vector units.[7]


Larrabee includes one major fixed-function graphics hardware feature: texture sampling units. These perform trilinear and anisotropic filtering and texture decompression.[7]


Larrabee has a 1024-bit (512-bit each way) ring bus for communication between cores and to memory.[7] This bus can be configured in two modes to support Larrabee products with 16 cores or more, or fewer than 16 cores.[9]


Larrabee includes explicit cache control instructions to reduce cache thrashing during streaming operations which only read/write data once.[7] Explicit prefetching into L2 or L1 cache is also supported.


Each core supports 4-way simultaneous multithreading, with 4 copies of each processor register.[7]

Theoretically Larrabee's x86 processor cores can run existing PC software; even operating systems. However, Larrabee's video card will not include all the features of a PC-compatible motherboard, so PC operating systems and applications will not run without modifications. A different version of Larrabee might sit in motherboard CPU sockets using QuickPath[10], but Intel has not yet announced plans for this. Though Larrabee Native's C/C++ compiler includes auto-vectorization and many applications can execute correctly after recompiling, maximum efficiency may require code optimization using C++ vector intrinsics or inline Larrabee assembly code.[7] However, as in all GPGPU, not all software benefits from utilization of a vector processing unit.

[edit] Comparison with the Cell Broadband Engine
Larrabee's philosophy of using many small, simple cores is similar to the ideas behind the Cell processor. There are some further commonalities, such as the use of a high-bandwidth ring bus to communicate between cores.[7] However, there are many significant differences in implementation which should make programming Larrabee simpler.

The Cell processor includes one main processor which controls many smaller processors. Additionally, the main processor can run an operating system. In contrast, all of Larrabee's cores are the same, and the Larrabee is not expected to run an OS.


Each computer core in the Cell (SPE) has a local store, for which explicit (DMA) operations are used for all accesses to DRAM. Ordinary reads/writes to DRAM are not allowed. In Larrabee, all on-chip and off-chip memories are under automatically-managed coherent cache hierarchy, so that its cores virtually share a uniform memory space through standard load/store instructions. However, Larrabee cores each have 256K of local L2 cache, and other L2 segments take longer to access, which is somewhat similar in principle to the Cell SPUs.[7].


Because of the cache coherency noted above, each program running in Larrabee has virtually a large linear memory just as in traditional general-purpose CPU; whereas an application for Cell should be programmed taking into consideration limited memory footprint of the local store associated with each SPE (for details see this article) but with theoretically higher bandwidth. However, since local L2 is faster to access, an advantage can still be gained from using Cell-style programming methods.


Cell uses DMA for data transfer to/from on-chip local memories, which has a merit in flexibility and throughput; whereas Larrabee uses special instructions for cache manipulation (notably cache eviction hints and pre-fetch instructions), which has a merit in that it can maintain cache coherence (hence the standard memory hierarchy) while boosting performance for e.g. rendering pipelines and other stream-like computation.[7].


Each compute core in the Cell runs only one thread at a time, in-order. A core in Larrabee runs up to four threads. Larrabee's hyperthreading helps hide latencies and compensates for lack of out-of-order execution.


[edit] Comparison with Intel GMA
Intel currently sells a line of GPUs under the Intel GMA brand. These chips are not sold separately but are integrated onto motherboards. Though the low cost and power consumption of Intel GMA chips make them suitable for small laptops and less demanding tasks, they lack the 3D graphics processing power to compete with NVIDIA and AMD/ATI for a share of the high-end gaming computer market, the HPC market, or a place in popular video game consoles. In contrast, Larrabee will be sold as a discrete GPU, separate from motherboards, and is expected to perform well enough for consideration in the next generation of video game consoles.[11]
The team working on Larrabee is separate from the Intel GMA team. The hardware is being designed by Intel's Hillsboro, Oregon design team, whose last major design was the Nehalem. The software and drivers are being written by a newly-formed team. The 3D stack specifically is being written by developers at RAD Game Tools (including Michael Abrash).[12]

[edit] Preliminary performance data




Benchmarking results from the recent SIGGRAPH paper, showing performance as an approximate linear function of the number of processing cores.


Intel's SIGGRAPH 2008 paper describes cycle-accurate simulations (limitations of memory, caches and texture units was included) of Larrabee's projected performance.[7] Graphs show how many 1 GHz Larrabee cores are required to maintain 60 FPS at 1600x1200 resolution in several popular games. Roughly 25 cores are required for Gears of War with no antialiasing, 25 cores for F.E.A.R with 4x antialiasing, and 10 cores for Half-Life 2: Episode 2 with 4x antialiasing. It is likely that Larrabee will run faster than 1 GHz, so these numbers do not represent actual Larrabee cores, rather virtual timeslices of such.[13] Another graph shows that performance on these games scales nearly linearly with the number of cores up to 32 cores. At 48 cores the performance drops to 90% of what would be expected if the linear relationship continued.
A June 2007 PC Watch article suggests that the first Larrabee chips will feature 32 x86 processor cores and come out in late 2009, fabricated on a 45 nanometer process. Chips with a few defective cores due to yield issues will be sold as a 24-core version. Later in 2010 Larrabee will be shrunk for a 32 nanometer fabrication process which will enable a 48 core version.[14]
Fudzilla has posted several short articles about Larrabee, claiming that Larrabee may have a TDP as large as 300W,[15] that Larrabee will use a 12-layer PCB and has a cooling system that "is meant to look similar to what you can find on high-end Nvidia cards today,"[16] that Larrabee will use GDDR5 memory, and that it is targeted to have 2 single-precision teraflops of computing power.[17]
The last statement of performance can be calculated (theoretically this is maximum possible performance) as follows: 32 cores x 16 single-precision float SIMD per core x 2 FLOPS (fused multiply-add) x 2GHz per core = 2 TFLOPS

[edit] See also

Intel740
Intel GMA
x86 architecture
x86-64
P5
List of Intel CPU microarchitectures


[edit] References


^ a b "First Details on a Future Intel Design Codenamed 'Larrabee'". Intel. http://www.intel.com/pressroom/archive/releases/20080804fact.htm. Retrieved on 2008-09-01. 
^ "Exclusive: Jon Peddie predicts great second half of 2009 for graphics market". Hexus. http://channel.hexus.net/content/item.php?item=17124&page=5. 
^ Stokes, Jon. "Intel picks up gaming physics engine for forthcoming GPU product". Ars Technica. http://arstechnica.com/news.ars/post/20070917-intel-picks-up-gaming-physics-engine-for-forthcoming-gpu-product.html. Retrieved on 2007-09-17. 
^ Stokes, Jon. "Clearing up the confusion over Intel's Larrabee". Ars Technica. http://arstechnica.com/articles/paedia/hardware/clearing-up-the-confusion-over-intels-larrabee.ars. Retrieved on 2007-06-01. 
^ [1]
^ NVISION 08, "Larrabee like a GPU from 2006"
^ a b c d e f g h i j k l m n o "Larrabee: A Many-Core x86 Architecture for Visual Computing". Intel. doi:10.1145/1399504.1360617. http://software.intel.com/file/2824/. Retrieved on 2008-08-06. 
^ "Intel's Larrabee GPU based on secret Pentagon tech, sorta [Updated"]. Ars Technica. http://arstechnica.com/news.ars/post/20080708-intels-larrabee-gpu-based-on-secret-pentagon-tech-sorta.html. Retrieved on 2008-08-06. 
^ Glaskowsky, Peter. "Intel's Larrabee--more and less than meets the eye". CNET. http://news.cnet.com/8301-13512_3-10006184-23.html. Retrieved on 2008-08-20. 
^ Stokes, Jon. "Clearing up the confusion over Intel's Larrabee, part II". Ars Technica. http://arstechnica.com/news.ars/post/20070604-clearing-up-the-confusion-over-intels-larrabee-part-ii.html. Retrieved on 2008-01-16. 
^ Chris Leyton (2008-08-13). "Intel's Larrabee Shaping Up For Next-Gen Consoles?". http://www.totalvideogames.com/news/Intels_Larrabee_Shaping_Up_For_Next-Gen_Consoles_13643_6321_0.htm. Retrieved on 2008-08-24. 
^ AnandTech: Intel's Larrabee Architecture Disclosure: A Calculated First Move
^ Steve Seguin (August 20, 2008). "Intel's 'Larrabee' to Shakeup AMD, Nvidia". Tom's Hardware. http://www.tomshardware.com/news/intel-larrabee-idf,6210.html. Retrieved on 2008-08-24. 
^ "Intel is promoting the 32 core CPU "Larrabee"". pc.watch.impress.co.jp. http://pc.watch.impress.co.jp/docs/2007/0611/kaigai364.htm. Retrieved on 2008-08-06. (Japanese)translation
^ "Larrabee to launch at 300W TDP". fudzilla.com. http://www.fudzilla.com/index.php?option=com_content&task=view&id=7651&Itemid=1. Retrieved on 2008-08-06. 
^ "Larrabee will use a 12-layer PCB". fudzilla.com. http://www.fudzilla.com/index.php?option=com_content&task=view&id=8435&Itemid=1. Retrieved on 2008-08-06. 
^ "Larrabee will use GDDR5 memory". fudzilla.com. http://www.fudzilla.com/index.php?option=com_content&task=view&id=8460&Itemid=1. Retrieved on 2008-08-06. 



[edit] External links

A First Look at the Larrabee New Instructions (LRBni)
C++ implementation of the Larrabee new instructions
Game Physics Performance on Larrabee
Intel fact sheet about Larrabee
Intel's SIGGRAPH 2008 paper on Larrabee
Techgage.com - Discusses how Larrabee differs from normal GPUs, includes block diagram illustration
Intel's Larrabee Architecture Disclosure: A Calculated First Move








v • d • e

Intel processors





Discontinued





pre-8086


4004 · 4040 · 8008 · 8080 · 8085







x86 (16 bit)


8086 · 8088 · 80186 · 80188 · 80286







x86/IA32 (32 bit)


80386 · 80486 · Pentium · Pentium Pro · Pentium II · Pentium III · Pentium 4 · Pentium M · Core · Celeron M · Celeron D







x86-64/EM64T (64 bit)


Pentium 4 (Some) · Pentium D · Pentium Extreme Edition · Celeron D (Some)







Other


Itanium — iAPX 432 — RISC: i860 · i960 · XScale — Microcontrollers: 8048 · 8051 · MCS-96









Current

Celeron · Pentium Dual-Core · Core 2 · A100 · Atom · Xeon · Itanium  · Core i7






Upcoming

Tukwila · Tolapai · Moorestown  · Core i5






Lists

CPU sockets · Chipsets · Microarchitectures · Processors · Codenames
Atom · Celeron · Core · Core 2 · Core i7 · Itanium · Pentium II · Pentium III · Pentium 4 · Pentium D · Pentium Dual-Core · Pentium M · Xeon
Future Celeron · Future Pentium · Future Core 2 · Future Core i7 · Future Xeon






Microarchitectures





Past and present


P5 · P6 · NetBurst · Core · Nehalem







Future


Larrabee · Sandy Bridge · Haswell












Retrieved from "http://en.wikipedia.org/wiki/Larrabee_(GPU)"
Categories: Upcoming chips | Intel x86 microprocessors | Intel microprocessors | Video cards 






Views


Article
Discussion
Edit this page
History 



Personal tools


Log in / create account






 if (window.isMSIE55) fixalpha(); 

Navigation


Main page
Contents
Featured content
Current events
Random article




Search




 
				




Interaction


About Wikipedia
Community portal
Recent changes
Contact Wikipedia
Donate to Wikipedia
Help




Toolbox


What links here
Related changes
Upload file
Special pages
Printable version Permanent linkCite this page 



Languages


Deutsch
Français
Italiano
日本語
Русский
Suomi
Svenska
中文









 This page was last modified on 9 April 2009, at 13:28 (UTC).
All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)  Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S. registered 501(c)(3) tax-deductible nonprofit charity.
Privacy policy
About Wikipedia
Disclaimers



if (window.runOnloadHook) runOnloadHook();
